{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent System with Genie + LLM Summarization\n",
        "\n",
        "This notebook creates a multi-agent system where:\n",
        "1. **Genie Agent** provides structured data (tables, statistics)\n",
        "2. **Supervisor Agent** (Llama 3.1) creates natural language summaries\n",
        "3. **Output includes BOTH** the table and the summary\n",
        "\n",
        "## Prerequisites\n",
        "- Genie Space created and configured\n",
        "- Databricks serving endpoint access\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U -qqq langgraph-supervisor==0.0.30 mlflow[databricks] databricks-langchain databricks-agents uv \n",
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Enhanced Multi-Agent System\n",
        "\n",
        "Key enhancement: The supervisor prompt explicitly instructs the LLM to:\n",
        "- Preserve structured data from Genie\n",
        "- Add a natural language summary\n",
        "- Provide insights and key findings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile agent.py\n",
        "import json\n",
        "from typing import Generator, Literal\n",
        "from uuid import uuid4\n",
        "\n",
        "import mlflow\n",
        "from databricks_langchain import (\n",
        "    ChatDatabricks,\n",
        "    DatabricksFunctionClient,\n",
        "    UCFunctionToolkit,\n",
        "    set_uc_function_client,\n",
        ")\n",
        "from databricks_langchain.genie import GenieAgent\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain.agents import create_agent\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "from langgraph_supervisor import create_supervisor\n",
        "from mlflow.pyfunc import ResponsesAgent\n",
        "from mlflow.types.responses import (\n",
        "    ResponsesAgentRequest,\n",
        "    ResponsesAgentResponse,\n",
        "    ResponsesAgentStreamEvent,\n",
        "    output_to_responses_items_stream,\n",
        "    to_chat_completions_input,\n",
        ")\n",
        "from pydantic import BaseModel\n",
        "\n",
        "client = DatabricksFunctionClient()\n",
        "set_uc_function_client(client)\n",
        "\n",
        "########################################\n",
        "# Agent Configuration Models\n",
        "########################################\n",
        "\n",
        "GENIE = \"genie\"\n",
        "\n",
        "\n",
        "class ServedSubAgent(BaseModel):\n",
        "    endpoint_name: str\n",
        "    name: str\n",
        "    task: Literal[\"agent/v1/responses\", \"agent/v1/chat\", \"agent/v2/chat\"]\n",
        "    description: str\n",
        "\n",
        "\n",
        "class Genie(BaseModel):\n",
        "    space_id: str\n",
        "    name: str\n",
        "    task: str = GENIE\n",
        "    description: str\n",
        "\n",
        "\n",
        "class InCodeSubAgent(BaseModel):\n",
        "    tools: list[str]\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "\n",
        "TOOLS = []\n",
        "\n",
        "\n",
        "def stringify_content(state):\n",
        "    \"\"\"Convert content to string format for processing\"\"\"\n",
        "    msgs = state[\"messages\"]\n",
        "    if isinstance(msgs[-1].content, list):\n",
        "        msgs[-1].content = json.dumps(msgs[-1].content, indent=4)\n",
        "    return {\"messages\": msgs}\n",
        "\n",
        "\n",
        "########################################\n",
        "# Create LangGraph Supervisor with Enhanced Summarization\n",
        "########################################\n",
        "\n",
        "\n",
        "def create_langgraph_supervisor(\n",
        "    llm: Runnable,\n",
        "    externally_served_agents: list[ServedSubAgent] = [],\n",
        "    in_code_agents: list[InCodeSubAgent] = [],\n",
        "):\n",
        "    agents = []\n",
        "    agent_descriptions = \"\"\n",
        "\n",
        "    # Process inline code agents\n",
        "    for agent in in_code_agents:\n",
        "        agent_descriptions += f\"- {agent.name}: {agent.description}\\n\"\n",
        "        uc_toolkit = UCFunctionToolkit(function_names=agent.tools)\n",
        "        TOOLS.extend(uc_toolkit.tools)\n",
        "        agents.append(create_agent(llm, tools=uc_toolkit.tools, name=agent.name))\n",
        "\n",
        "    # Process served endpoints and Genie Spaces\n",
        "    for agent in externally_served_agents:\n",
        "        agent_descriptions += f\"- {agent.name}: {agent.description}\\n\"\n",
        "        if isinstance(agent, Genie):\n",
        "            genie_agent = GenieAgent(\n",
        "                genie_space_id=agent.space_id,\n",
        "                genie_agent_name=agent.name,\n",
        "                description=agent.description,\n",
        "            )\n",
        "            genie_agent.name = agent.name\n",
        "            agents.append(genie_agent)\n",
        "        else:\n",
        "            model = ChatDatabricks(\n",
        "                endpoint=agent.endpoint_name, use_responses_api=\"responses\" in agent.task\n",
        "            )\n",
        "            # Disable streaming for subagents for ease of parsing\n",
        "            model._stream = lambda x: model._stream(**x, stream=False)\n",
        "            agents.append(\n",
        "                create_agent(\n",
        "                    model,\n",
        "                    tools=[],\n",
        "                    name=agent.name,\n",
        "                    post_model_hook=stringify_content,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # ENHANCED SUPERVISOR PROMPT - Returns 2-line summary + table\n",
        "    prompt = f\"\"\"You are a supervisor that analyzes workforce data and provides insights.\n",
        "\n",
        "When an agent returns data with a table, you MUST respond in this EXACT format:\n",
        "\n",
        "[Write EXACTLY 2 lines of summary with the key insight and specific numbers]\n",
        "\n",
        "[Include the complete table from the agent]\n",
        "\n",
        "RULES:\n",
        "- Summary must be EXACTLY 2 lines (not sentences, but 2 short lines)\n",
        "- First line: State the highest/most important finding with a specific number\n",
        "- Second line: Note one additional insight or comparison\n",
        "- Always preserve the complete table after your 2-line summary\n",
        "- Do NOT add extra text before or after\n",
        "\n",
        "Example:\n",
        "Sales department has the highest attrition rate at 15.2%, significantly above the 8.1% company average.\n",
        "Engineering maintains the strongest retention at 6.3%, indicating effective retention programs in technical roles.\n",
        "\n",
        "| Department  | Attrition Rate | Count |\n",
        "|-------------|----------------|-------|\n",
        "| Sales       | 15.2%          | 450   |\n",
        "| Engineering | 6.3%           | 520   |\n",
        "\n",
        "Available agents:\n",
        "{agent_descriptions}\"\"\"\n",
        "\n",
        "    return create_supervisor(\n",
        "        agents=agents,\n",
        "        model=llm,\n",
        "        prompt=prompt,\n",
        "        add_handoff_messages=False,\n",
        "        output_mode=\"full_history\",  # Keep full history to get both summary and table\n",
        "    ).compile()\n",
        "\n",
        "\n",
        "##########################################\n",
        "# Wrap LangGraph Supervisor as a ResponsesAgent\n",
        "##########################################\n",
        "\n",
        "\n",
        "class LangGraphResponsesAgent(ResponsesAgent):\n",
        "    def __init__(self, agent: CompiledStateGraph):\n",
        "        self.agent = agent\n",
        "\n",
        "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
        "        outputs = [\n",
        "            event.item\n",
        "            for event in self.predict_stream(request)\n",
        "            if event.type == \"response.output_item.done\"\n",
        "        ]\n",
        "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
        "\n",
        "    def predict_stream(\n",
        "        self,\n",
        "        request: ResponsesAgentRequest,\n",
        "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
        "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
        "        first_message = True\n",
        "        seen_ids = set()\n",
        "\n",
        "        for _, events in self.agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\"]):\n",
        "            new_msgs = [\n",
        "                msg\n",
        "                for v in events.values()\n",
        "                for msg in v.get(\"messages\", [])\n",
        "                if msg.id not in seen_ids\n",
        "            ]\n",
        "            if first_message:\n",
        "                seen_ids.update(msg.id for msg in new_msgs[: len(cc_msgs)])\n",
        "                new_msgs = new_msgs[len(cc_msgs) :]\n",
        "                first_message = False\n",
        "            else:\n",
        "                seen_ids.update(msg.id for msg in new_msgs)\n",
        "                node_name = tuple(events.keys())[0]\n",
        "                yield ResponsesAgentStreamEvent(\n",
        "                    type=\"response.output_item.done\",\n",
        "                    item=self.create_text_output_item(\n",
        "                        text=f\"<name>{node_name}</name>\", id=str(uuid4())\n",
        "                    ),\n",
        "                )\n",
        "            if len(new_msgs) > 0:\n",
        "                yield from output_to_responses_items_stream(new_msgs)\n",
        "\n",
        "\n",
        "#######################################################\n",
        "# Configure Foundation Model and Sub-Agents\n",
        "#######################################################\n",
        "\n",
        "# Foundation model for supervisor (will generate summaries)\n",
        "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
        "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
        "\n",
        "# Configure your Genie Space\n",
        "EXTERNALLY_SERVED_AGENTS = [\n",
        "    Genie(\n",
        "        space_id=\"01f0c9f705201d14b364f5daf28bb639\",  # TODO: Update with your Genie Space ID\n",
        "        name=\"talent_genie\",\n",
        "        description=\"Analyzes talent stability, mobility patterns, attrition risk, and workforce trends. Provides structured data including statistics, tables, and detailed breakdowns by department, role, tenure, and other dimensions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Optional: Add UC function-calling agents\n",
        "IN_CODE_AGENTS = []\n",
        "\n",
        "#################################################\n",
        "# Create Supervisor and Set Up MLflow\n",
        "#################################################\n",
        "\n",
        "supervisor = create_langgraph_supervisor(llm, EXTERNALLY_SERVED_AGENTS, IN_CODE_AGENTS)\n",
        "\n",
        "mlflow.langchain.autolog()\n",
        "AGENT = LangGraphResponsesAgent(supervisor)\n",
        "mlflow.models.set_model(AGENT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Agent\n",
        "\n",
        "Test the agent locally before deploying. You should see:\n",
        "1. **Summary** from the supervisor (natural language insights)\n",
        "2. **Table** from Genie (structured data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agent import AGENT\n",
        "\n",
        "# Test with a question that will require Genie to query data\n",
        "input_example = {\n",
        "    \"input\": [\n",
        "        {\"role\": \"user\", \"content\": \"Which department has the highest attrition rate?\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Get the response\n",
        "response = AGENT.predict(input_example)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test streaming to see the flow\n",
        "print(\"=\" * 80)\n",
        "print(\"STREAMING OUTPUT (shows agent handoffs and responses)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for event in AGENT.predict_stream(input_example):\n",
        "    output = event.model_dump(exclude_none=True)\n",
        "    \n",
        "    # Extract and display content\n",
        "    if 'item' in output and 'content' in output['item']:\n",
        "        for content_item in output['item']['content']:\n",
        "            if 'text' in content_item:\n",
        "                text = content_item['text']\n",
        "                \n",
        "                # Highlight agent names\n",
        "                if text.startswith('<name>'):\n",
        "                    print(f\"\\n{'='*60}\")\n",
        "                    print(f\"âžœ Agent: {text}\")\n",
        "                    print(f\"{'='*60}\\n\")\n",
        "                else:\n",
        "                    print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log the Agent to MLflow\n",
        "\n",
        "Log the agent with automatic authentication for Databricks resources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from agent import EXTERNALLY_SERVED_AGENTS, LLM_ENDPOINT_NAME, TOOLS, Genie\n",
        "from databricks_langchain import UnityCatalogTool, VectorSearchRetrieverTool\n",
        "from mlflow.models.resources import (\n",
        "    DatabricksFunction,\n",
        "    DatabricksGenieSpace,\n",
        "    DatabricksServingEndpoint,\n",
        "    DatabricksSQLWarehouse,\n",
        "    DatabricksTable\n",
        ")\n",
        "from pkg_resources import get_distribution\n",
        "\n",
        "# Configure resources for automatic authentication\n",
        "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
        "\n",
        "# Add SQL Warehouse and tables for Genie Space\n",
        "# TODO: Update these with your actual warehouse and table names\n",
        "resources.append(DatabricksSQLWarehouse(warehouse_id=\"148ccb90800933a1\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.fact_attrition_snapshots\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.dim_employees\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.fact_compensation\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.fact_performance\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.fact_role_history\"))\n",
        "\n",
        "# Add UC function tools if any\n",
        "for tool in TOOLS:\n",
        "    if isinstance(tool, VectorSearchRetrieverTool):\n",
        "        resources.extend(tool.resources)\n",
        "    elif isinstance(tool, UnityCatalogTool):\n",
        "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
        "\n",
        "# Add Genie Space\n",
        "for agent in EXTERNALLY_SERVED_AGENTS:\n",
        "    if isinstance(agent, Genie):\n",
        "        resources.append(DatabricksGenieSpace(genie_space_id=agent.space_id))\n",
        "    else:\n",
        "        resources.append(DatabricksServingEndpoint(endpoint_name=agent.endpoint_name))\n",
        "\n",
        "# Log the model\n",
        "with mlflow.start_run():\n",
        "    logged_agent_info = mlflow.pyfunc.log_model(\n",
        "        name=\"agent\",\n",
        "        python_model=\"agent.py\",\n",
        "        resources=resources,\n",
        "        pip_requirements=[\n",
        "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
        "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
        "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
        "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
        "            f\"langgraph-supervisor=={get_distribution('langgraph-supervisor').version}\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "print(f\"âœ… Model logged successfully!\")\n",
        "print(f\"Run ID: {logged_agent_info.run_id}\")\n",
        "print(f\"Model URI: {logged_agent_info.model_uri}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register to Unity Catalog\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.set_registry_uri(\"databricks-uc\")\n",
        "\n",
        "# TODO: Update these with your catalog, schema, and model name\n",
        "catalog = \"akash_s_demo\"\n",
        "schema = \"talent\"\n",
        "model_name = \"mobility_attrition_with_summary\"\n",
        "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
        "\n",
        "# Register the model\n",
        "uc_registered_model_info = mlflow.register_model(\n",
        "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
        ")\n",
        "\n",
        "print(f\"âœ… Model registered to Unity Catalog!\")\n",
        "print(f\"Model: {UC_MODEL_NAME}\")\n",
        "print(f\"Version: {uc_registered_model_info.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the Agent\n",
        "\n",
        "Deploy the agent to a serving endpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from databricks import agents\n",
        "\n",
        "# Deploy the agent\n",
        "deployment_info = agents.deploy(\n",
        "    UC_MODEL_NAME, \n",
        "    uc_registered_model_info.version,\n",
        "    tags={\"enhanced\": \"with_summary\"},\n",
        "    deploy_feedback_model=False\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸš€ DEPLOYMENT INITIATED\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nYour agent with enhanced summarization is being deployed!\")\n",
        "print(\"\\nðŸ“Š What to expect:\")\n",
        "print(\"  â€¢ Natural language summaries from Llama 3.1\")\n",
        "print(\"  â€¢ Structured tables from Genie\")\n",
        "print(\"  â€¢ Both in a single response\")\n",
        "print(\"\\nThis deployment can take up to 15 minutes.\")\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Output\n",
        "\n",
        "### Question: \"Give me attrition rates for each BU\"\n",
        "\n",
        "**What you'll get:**\n",
        "\n",
        "```\n",
        "Sales department has the highest attrition rate at 15.2%, significantly above the 8.1% company average.\n",
        "Engineering maintains the strongest retention at 6.3%, indicating effective retention programs in technical roles.\n",
        "\n",
        "| Department  | Attrition Rate | Employee Count | Avg Tenure |\n",
        "|-------------|----------------|----------------|------------|\n",
        "| Sales       | 15.2%          | 450            | 2.3 years  |\n",
        "| Support     | 12.8%          | 320            | 2.8 years  |\n",
        "| Marketing   | 10.5%          | 180            | 3.2 years  |\n",
        "| Operations  | 9.2%           | 280            | 3.8 years  |\n",
        "| Engineering | 6.3%           | 520            | 4.5 years  |\n",
        "```\n",
        "\n",
        "**In your Dash app, this will display as:**\n",
        "- âœ… **2-line summary** at the top (easy to read)\n",
        "- âœ… **Formatted table** below (with proper styling)\n",
        "- âœ… **Agent badge** showing which agent answered\n",
        "\n",
        "## Key Features\n",
        "\n",
        "âœ… **Concise** - Exactly 2 lines of summary, no fluff  \n",
        "âœ… **Specific** - Uses actual numbers from the data  \n",
        "âœ… **Complete** - Full table preserved for detailed analysis  \n",
        "âœ… **Frontend Ready** - Dash app already parses and displays this format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

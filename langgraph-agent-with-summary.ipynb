{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent System with Genie + LLM Summarization\n",
        "\n",
        "This notebook creates a multi-agent system where:\n",
        "1. **Genie Agent** provides structured data (tables, statistics)\n",
        "2. **Supervisor Agent** (Llama 3.1) creates natural language summaries\n",
        "3. **Output includes BOTH** the table and the summary\n",
        "\n",
        "## Prerequisites\n",
        "- Genie Space created and configured\n",
        "- Databricks serving endpoint access\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U -qqq langgraph-supervisor==0.0.30 mlflow[databricks] databricks-langchain databricks-agents databricks-ai-bridge uv \n",
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Multi-Agent System with Intelligent Routing\n",
        "\n",
        "### Graph Architecture (Nodes, Edges, and Conditional Routing):\n",
        "\n",
        "```\n",
        "                    START\n",
        "                      ↓\n",
        "            ┌──────────────────┐\n",
        "            │Supervisor Router │  ← Classifies question\n",
        "            └────────┬─────────┘\n",
        "                     │\n",
        "        ┌────────────┴────────────┐\n",
        "        │                         │\n",
        "     TALENT                    OTHER\n",
        "        │                         │\n",
        "        ↓                         ↓\n",
        "  ┌──────────┐            \"No data available\"\n",
        "  │  Genie   │                   │\n",
        "  └────┬─────┘                   ↓\n",
        "       │                        END\n",
        "       ↓\n",
        "┌─────────────────┐\n",
        "│Supervisor       │  ← Creates summary + table\n",
        "│Summarizer       │\n",
        "└────────┬────────┘\n",
        "         │\n",
        "         ↓\n",
        "        END\n",
        "```\n",
        "\n",
        "### Key Features:\n",
        "- ✅ **Intelligent routing**: Supervisor decides if question is talent-related\n",
        "- ✅ **Conditional logic**: Only calls Genie for talent questions\n",
        "- ✅ **Efficient**: Avoids unnecessary API calls for off-topic questions  \n",
        "- ✅ **Guaranteed format**: All Genie responses get 2-line summary + table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile agent.py\n",
        "import json\n",
        "from typing import Generator, Literal\n",
        "from uuid import uuid4\n",
        "\n",
        "import mlflow\n",
        "from databricks_langchain import (\n",
        "    ChatDatabricks,\n",
        "    DatabricksFunctionClient,\n",
        "    UCFunctionToolkit,\n",
        "    set_uc_function_client,\n",
        ")\n",
        "from databricks_ai_bridge import ModelServingUserCredentials  # OBO authentication\n",
        "from databricks.sdk import WorkspaceClient  # For Genie OBO with explicit credentials\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain.agents import create_agent\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "from langgraph_supervisor import create_supervisor\n",
        "from mlflow.pyfunc import ResponsesAgent\n",
        "from mlflow.types.responses import (\n",
        "    ResponsesAgentRequest,\n",
        "    ResponsesAgentResponse,\n",
        "    ResponsesAgentStreamEvent,\n",
        "    output_to_responses_items_stream,\n",
        "    to_chat_completions_input,\n",
        ")\n",
        "from pydantic import BaseModel\n",
        "\n",
        "########################################\n",
        "# Agent Configuration Models\n",
        "########################################\n",
        "\n",
        "GENIE = \"genie\"\n",
        "\n",
        "\n",
        "class ServedSubAgent(BaseModel):\n",
        "    endpoint_name: str\n",
        "    name: str\n",
        "    task: Literal[\"agent/v1/responses\", \"agent/v1/chat\", \"agent/v2/chat\"]\n",
        "    description: str\n",
        "\n",
        "\n",
        "class Genie(BaseModel):\n",
        "    space_id: str\n",
        "    name: str\n",
        "    task: str = GENIE\n",
        "    description: str\n",
        "\n",
        "\n",
        "class InCodeSubAgent(BaseModel):\n",
        "    tools: list[str]\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "\n",
        "def stringify_content(state):\n",
        "    \"\"\"Convert content to string format for processing\"\"\"\n",
        "    msgs = state[\"messages\"]\n",
        "    if isinstance(msgs[-1].content, list):\n",
        "        msgs[-1].content = json.dumps(msgs[-1].content, indent=4)\n",
        "    return {\"messages\": msgs}\n",
        "\n",
        "\n",
        "def query_genie_with_obo(workspace_client: WorkspaceClient, space_id: str, question: str) -> str:\n",
        "    \"\"\"\n",
        "    Query Genie Space using OBO credentials via direct API call.\n",
        "    \n",
        "    This bypasses the GenieAgent wrapper to ensure user credentials are used.\n",
        "    \n",
        "    Args:\n",
        "        workspace_client: WorkspaceClient with OBO credentials\n",
        "        space_id: Genie Space ID\n",
        "        question: User's question\n",
        "    \n",
        "    Returns:\n",
        "        Genie's response as string\n",
        "    \"\"\"\n",
        "    import time\n",
        "    \n",
        "    try:\n",
        "        # Start a conversation with Genie using OBO WorkspaceClient\n",
        "        conversation = workspace_client.genie.start_conversation(\n",
        "            space_id=space_id,\n",
        "            content=question\n",
        "        )\n",
        "        \n",
        "        conversation_id = conversation.conversation_id\n",
        "        message_id = conversation.message_id\n",
        "        \n",
        "        # Poll for results\n",
        "        max_wait = 60  # seconds\n",
        "        start_time = time.time()\n",
        "        \n",
        "        while time.time() - start_time < max_wait:\n",
        "            # Get message status\n",
        "            message = workspace_client.genie.get_message(\n",
        "                space_id=space_id,\n",
        "                conversation_id=conversation_id,\n",
        "                message_id=message_id\n",
        "            )\n",
        "            \n",
        "            if message.status == \"COMPLETED\":\n",
        "                # Get the response attachments\n",
        "                if message.attachments:\n",
        "                    # Genie returns data in attachments\n",
        "                    response_parts = []\n",
        "                    for attachment in message.attachments:\n",
        "                        if hasattr(attachment, 'text') and attachment.text:\n",
        "                            response_parts.append(attachment.text.content)\n",
        "                        elif hasattr(attachment, 'query') and attachment.query:\n",
        "                            # Include query results if available\n",
        "                            if hasattr(attachment.query, 'result_data'):\n",
        "                                response_parts.append(str(attachment.query.result_data))\n",
        "                    \n",
        "                    return \"\\n\\n\".join(response_parts) if response_parts else message.content\n",
        "                \n",
        "                return message.content or \"No response from Genie\"\n",
        "            \n",
        "            elif message.status == \"FAILED\":\n",
        "                error_msg = f\"Genie query failed: {message.error if hasattr(message, 'error') else 'Unknown error'}\"\n",
        "                print(f\"ERROR: {error_msg}\")\n",
        "                return f\"Error querying Genie: {error_msg}\"\n",
        "            \n",
        "            # Still processing, wait and retry\n",
        "            time.sleep(2)\n",
        "        \n",
        "        return \"Genie query timed out after 60 seconds\"\n",
        "    \n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error calling Genie API: {str(e)}\"\n",
        "        print(f\"ERROR: {error_msg}\")\n",
        "        return error_msg\n",
        "\n",
        "\n",
        "########################################\n",
        "# Create Custom LangGraph with Explicit Nodes and Edges\n",
        "########################################\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "\n",
        "\n",
        "# Define the state structure\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, operator.add]\n",
        "    next_step: str  # Track routing decision\n",
        "\n",
        "\n",
        "def create_langgraph_with_nodes(\n",
        "    llm: Runnable,\n",
        "    workspace_client: WorkspaceClient,  # OBO-enabled WorkspaceClient\n",
        "    externally_served_agents: list[ServedSubAgent] = [],\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a LangGraph with intelligent routing:\n",
        "    - User Question → Supervisor Router (decides if talent-related)\n",
        "    - If YES → Genie Node (gets data with OBO) → Supervisor Summarizer (creates summary)\n",
        "    - If NO → Direct response (no data available)\n",
        "    - Final Response → END (returns to user)\n",
        "    \n",
        "    Args:\n",
        "        llm: Language model for routing and summarization\n",
        "        workspace_client: OBO-enabled WorkspaceClient for Genie API calls\n",
        "        externally_served_agents: List of served agents (Genie)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Find Genie space configuration\n",
        "    genie_space_id = None\n",
        "    genie_name = None\n",
        "    genie_description = None\n",
        "    \n",
        "    for agent in externally_served_agents:\n",
        "        if isinstance(agent, Genie):\n",
        "            genie_space_id = agent.space_id\n",
        "            genie_name = agent.name\n",
        "            genie_description = agent.description\n",
        "            break\n",
        "    \n",
        "    if not genie_space_id:\n",
        "        raise ValueError(\"Genie agent configuration is required\")\n",
        "    \n",
        "    # Define Supervisor Router node (decides if talent-related)\n",
        "    def supervisor_router(state: AgentState):\n",
        "        \"\"\"Supervisor router - determines if question is about talent/workforce data\"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        \n",
        "        # Get the user's question\n",
        "        user_question = \"\"\n",
        "        for msg in messages:\n",
        "            if hasattr(msg, 'content') and msg.content:\n",
        "                if isinstance(msg, HumanMessage) or (hasattr(msg, 'role') and msg.role == 'user'):\n",
        "                    user_question = msg.content\n",
        "                    break\n",
        "        \n",
        "        print(f\"DEBUG Router - User question: {user_question[:100]}\")\n",
        "        \n",
        "        # Use LLM with few-shot examples for robust classification\n",
        "        routing_prompt = f\"\"\"You are a routing assistant for a TALENT & WORKFORCE ANALYTICS chatbot.\n",
        "\n",
        "This chatbot can ONLY answer questions about:\n",
        "✓ Workforce data (employees, headcount, demographics)\n",
        "✓ Organizational structure (departments, business units, teams, managers)  \n",
        "✓ Attrition & retention (turnover, exits, resignations, churn)\n",
        "✓ Employee mobility (promotions, transfers, career paths)\n",
        "✓ HR metrics (compensation, performance, tenure, reviews)\n",
        "✓ Workforce trends and analytics\n",
        "\n",
        "The chatbot CANNOT answer questions about:\n",
        "✗ General knowledge, facts, or trivia\n",
        "✗ Current events, news, or weather\n",
        "✗ Products, services, or customer data (unless about employees)\n",
        "✗ Technical support or IT issues\n",
        "✗ Anything unrelated to employees/workforce\n",
        "\n",
        "TASK: Classify if the following question can be answered with workforce/talent data.\n",
        "\n",
        "Question: \"{user_question}\"\n",
        "\n",
        "CLASSIFICATION EXAMPLES (learn the pattern):\n",
        "\n",
        "\"Which department has highest attrition rate?\" → TALENT (clearly about workforce)\n",
        "\"Give me BU level attrition details\" → TALENT (BU = business unit, workforce metric)\n",
        "\"Show me employee turnover\" → TALENT (employee metric)\n",
        "\"Attrition by team\" → TALENT (workforce analytics)\n",
        "\"What percentage of people left?\" → TALENT (people = employees)  \n",
        "\"Details about organizational structure\" → TALENT (org data)\n",
        "\"How many staff in each division?\" → TALENT (headcount query)\n",
        "\"Show me the data\" → TALENT (assume workforce data in this context)\n",
        "\"Give me details\" → TALENT (assume talent details in this context)\n",
        "\"What are the rates?\" → TALENT (likely workforce rates)\n",
        "\"What's the weather today?\" → OTHER (unrelated to workforce)\n",
        "\"Tell me about Python programming\" → OTHER (technical, not workforce)\n",
        "\"What is a good restaurant?\" → OTHER (unrelated to talent)\n",
        "\n",
        "DECISION RULE:\n",
        "- If the question could plausibly be asking for workforce/talent/organizational data → TALENT\n",
        "- If the question is clearly about a non-workforce topic → OTHER\n",
        "- When in doubt, choose TALENT (better to try and fail than miss valid questions)\n",
        "\n",
        "Your classification (respond with ONLY the word \"TALENT\" or \"OTHER\"):\"\"\"\n",
        "        \n",
        "        routing_response = llm.invoke([HumanMessage(content=routing_prompt)])\n",
        "        decision = routing_response.content.strip().upper()\n",
        "        \n",
        "        print(f\"DEBUG Router - Decision: {decision}\")\n",
        "        \n",
        "        if \"TALENT\" in decision:\n",
        "            # Route to Genie\n",
        "            return {\"next_step\": \"genie\", \"messages\": []}\n",
        "        else:\n",
        "            # Return message saying we don't have data\n",
        "            response_msg = AIMessage(\n",
        "                content=\"I'm specialized in talent and workforce analytics. I don't have information about that topic. Please ask me questions about attrition, employee mobility, retention, or workforce trends.\",\n",
        "                name=\"supervisor\"\n",
        "            )\n",
        "            return {\"next_step\": \"end\", \"messages\": [response_msg]}\n",
        "    \n",
        "    # Define Genie node function (uses direct API with OBO)\n",
        "    def genie_node(state: AgentState):\n",
        "        \"\"\"Genie node - queries data using OBO credentials via direct API\"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        \n",
        "        print(f\"DEBUG Genie - Input messages: {len(messages)}\")\n",
        "        \n",
        "        # Extract user's question from messages\n",
        "        user_question = \"\"\n",
        "        for msg in messages:\n",
        "            if hasattr(msg, 'content') and msg.content:\n",
        "                if isinstance(msg, HumanMessage) or (hasattr(msg, 'role') and msg.role == 'user'):\n",
        "                    user_question = msg.content\n",
        "                    break\n",
        "        \n",
        "        if not user_question:\n",
        "            print(\"ERROR Genie - No user question found!\")\n",
        "            return {\"messages\": [AIMessage(content=\"Error: No question provided to Genie\", name=\"genie\")]}\n",
        "        \n",
        "        print(f\"DEBUG Genie - Querying with: {user_question[:100]}\")\n",
        "        \n",
        "        # Query Genie API directly with OBO credentials\n",
        "        genie_response = query_genie_with_obo(\n",
        "            workspace_client=workspace_client,\n",
        "            space_id=genie_space_id,\n",
        "            question=user_question\n",
        "        )\n",
        "        \n",
        "        print(f\"DEBUG Genie - Response length: {len(genie_response)}\")\n",
        "        print(f\"DEBUG Genie - Response preview: {genie_response[:200]}\")\n",
        "        \n",
        "        # Return as AIMessage\n",
        "        genie_message = AIMessage(content=genie_response, name=\"genie\")\n",
        "        return {\"messages\": [genie_message]}\n",
        "    \n",
        "    # Helper function to clean pandas-formatted markdown tables\n",
        "    def clean_pandas_table(text):\n",
        "        \"\"\"\n",
        "        Remove pandas index column from markdown tables.\n",
        "        Converts: |    | col1 | col2 |  →  | col1 | col2 |\n",
        "                  |---:|:-----|------|      |:-----|------|\n",
        "                  |  0 | val1 | val2 |      | val1 | val2 |\n",
        "        \"\"\"\n",
        "        import re\n",
        "        \n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "        \n",
        "        for line in lines:\n",
        "            if '|' in line:\n",
        "                # Split by pipe and strip whitespace\n",
        "                cells = [cell.strip() for cell in line.split('|')]\n",
        "                \n",
        "                # Check if this is a table line (has multiple cells)\n",
        "                if len(cells) >= 3:  # At least: ['', 'content', '']\n",
        "                    # Remove leading/trailing empty cells\n",
        "                    while cells and cells[0] == '':\n",
        "                        cells.pop(0)\n",
        "                    while cells and cells[-1] == '':\n",
        "                        cells.pop()\n",
        "                    \n",
        "                    # Check if this is a separator line (only dashes, colons, spaces)\n",
        "                    is_separator = cells and all(re.match(r'^[-:\\s]+$', cell) for cell in cells)\n",
        "                    \n",
        "                    # Check if first cell is pandas index (empty, numeric, or separator marker)\n",
        "                    if cells and (cells[0] == '' or \n",
        "                                  cells[0].isdigit() or \n",
        "                                  re.match(r'^\\s*\\d+\\s*$', cells[0]) or\n",
        "                                  (is_separator and re.match(r'^-+:?$', cells[0]))):\n",
        "                        # Remove the first cell (pandas index)\n",
        "                        cells = cells[1:]\n",
        "                    \n",
        "                    # Rebuild the line with clean cells\n",
        "                    if cells:\n",
        "                        cleaned_line = '| ' + ' | '.join(cells) + ' |'\n",
        "                        cleaned_lines.append(cleaned_line)\n",
        "                else:\n",
        "                    cleaned_lines.append(line)\n",
        "            else:\n",
        "                cleaned_lines.append(line)\n",
        "        \n",
        "        return '\\n'.join(cleaned_lines)\n",
        "    \n",
        "    # Define Supervisor Summarizer node (creates summary after Genie)\n",
        "    def supervisor_summarizer(state: AgentState):\n",
        "        \"\"\"Supervisor summarizer - creates 2-line summary + preserves Genie's table\"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        \n",
        "        # Get ALL messages - find the one from Genie (should be AI message after user message)\n",
        "        genie_response = \"\"\n",
        "        \n",
        "        # Look for the last AI message (from Genie)\n",
        "        for msg in reversed(messages):\n",
        "            if hasattr(msg, 'content') and msg.content:\n",
        "                # Check if it's an AI message and has actual content\n",
        "                if isinstance(msg, AIMessage) or (hasattr(msg, 'type') and msg.type == 'ai'):\n",
        "                    content = str(msg.content)\n",
        "                    # Skip if it's too short or empty\n",
        "                    if content and len(content.strip()) > 10:\n",
        "                        genie_response = content\n",
        "                        break\n",
        "        \n",
        "        # Debug: Print what we got from Genie\n",
        "        print(f\"DEBUG - Messages count: {len(messages)}\")\n",
        "        print(f\"DEBUG - Genie response length: {len(genie_response) if genie_response else 0}\")\n",
        "        if genie_response:\n",
        "            print(f\"DEBUG - Genie response preview: {genie_response[:200]}...\")\n",
        "        \n",
        "        if not genie_response or len(genie_response.strip()) < 10:\n",
        "            # If still no response, get the full state for debugging\n",
        "            error_msg = f\"No data received from Genie. Messages in state: {len(messages)}\"\n",
        "            print(f\"ERROR: {error_msg}\")\n",
        "            for i, msg in enumerate(messages):\n",
        "                print(f\"  Message {i}: type={type(msg).__name__}, has_content={hasattr(msg, 'content')}\")\n",
        "                if hasattr(msg, 'content'):\n",
        "                    content_preview = str(msg.content)[:100]\n",
        "                    print(f\"    Content preview: {content_preview}\")\n",
        "            return {\"messages\": [AIMessage(content=error_msg)]}\n",
        "        \n",
        "        # Check if Genie returned an error instead of data\n",
        "        if any(error_keyword in genie_response for error_keyword in [\"Error\", \"PERMISSION_DENIED\", \"FAILED\", \"failed with error\"]):\n",
        "            print(f\"ERROR: Genie returned error: {genie_response[:200]}\")\n",
        "            error_msg = \"I apologize, but I don't have access to the requested data. This may be due to data permissions or connectivity issues. Please contact your administrator if you believe you should have access to this information.\"\n",
        "            return {\"messages\": [AIMessage(content=error_msg, name=\"supervisor_summarizer\")]}\n",
        "        \n",
        "        # Clean up pandas-formatted tables (remove index column)\n",
        "        genie_response = clean_pandas_table(genie_response)\n",
        "        print(f\"DEBUG - Cleaned genie response length: {len(genie_response)}\")\n",
        "        print(f\"DEBUG - Cleaned genie response preview: {genie_response[:200]}...\")\n",
        "        \n",
        "        # Create supervisor prompt - be VERY explicit (NO EXAMPLES to avoid hallucination)\n",
        "        system_prompt = \"\"\"You are a data analyst. Your job is to write a 2-line summary and include the original table.\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "[Line 1: Key finding from the actual data provided]\n",
        "[Line 2: Second insight from the actual data provided]\n",
        "\n",
        "[PASTE THE COMPLETE ORIGINAL TABLE HERE EXACTLY AS PROVIDED]\n",
        "\n",
        "CRITICAL RULES:\n",
        "- Write EXACTLY 2 short lines analyzing ONLY the actual data provided below\n",
        "- NEVER make up data or use example data\n",
        "- Add blank line\n",
        "- Copy the COMPLETE original table unchanged\n",
        "- Use ONLY data from the table provided to you\n",
        "- That's it - nothing else\"\"\"\n",
        "        \n",
        "        # Create messages for LLM with explicit instruction\n",
        "        user_prompt = f\"\"\"Here is the data with a table:\n",
        "\n",
        "{genie_response}\n",
        "\n",
        "Instructions:\n",
        "1. Write 2 lines summarizing the key findings\n",
        "2. Include the complete table from above\n",
        "\n",
        "Your response:\"\"\"\n",
        "        \n",
        "        supervisor_messages = [\n",
        "            SystemMessage(content=system_prompt),\n",
        "            HumanMessage(content=user_prompt)\n",
        "        ]\n",
        "        \n",
        "        # Get summary from LLM\n",
        "        summary_response = llm.invoke(supervisor_messages)\n",
        "        \n",
        "        # Combine summary with original table to ensure table is preserved\n",
        "        final_response = summary_response.content\n",
        "        \n",
        "        # If the table isn't in the response, append it\n",
        "        if '|' not in final_response and '|' in genie_response:\n",
        "            print(\"DEBUG - Table not in LLM response, appending original table\")\n",
        "            final_response = f\"{final_response}\\n\\n{genie_response}\"\n",
        "        \n",
        "        print(f\"DEBUG Supervisor - Final response length: {len(final_response)}\")\n",
        "        print(f\"DEBUG Supervisor - Response preview: {final_response[:300]}\")\n",
        "        \n",
        "        # Create a message with explicit ID to ensure it's unique\n",
        "        summary_message = AIMessage(\n",
        "            content=final_response,\n",
        "            name=\"supervisor_summarizer\",\n",
        "            id=str(uuid4())  # Ensure unique ID\n",
        "        )\n",
        "        \n",
        "        print(f\"DEBUG Supervisor - Created message with ID: {summary_message.id}\")\n",
        "        \n",
        "        # Return as a single message with the summary content\n",
        "        return {\"messages\": [summary_message]}\n",
        "    \n",
        "    # Conditional edge function\n",
        "    def route_after_supervisor(state: AgentState):\n",
        "        \"\"\"Route based on supervisor's decision\"\"\"\n",
        "        next_step = state.get(\"next_step\", \"end\")\n",
        "        print(f\"DEBUG Routing - Next step: {next_step}\")\n",
        "        \n",
        "        if next_step == \"genie\":\n",
        "            return \"genie\"\n",
        "        else:\n",
        "            return END\n",
        "    \n",
        "    # Build the graph with conditional routing\n",
        "    workflow = StateGraph(AgentState)\n",
        "    \n",
        "    # Add nodes\n",
        "    workflow.add_node(\"supervisor_router\", supervisor_router)\n",
        "    workflow.add_node(\"genie\", genie_node)\n",
        "    workflow.add_node(\"supervisor_summarizer\", supervisor_summarizer)\n",
        "    \n",
        "    # Define edges with conditional routing\n",
        "    # START → Supervisor Router (decides if talent-related)\n",
        "    workflow.set_entry_point(\"supervisor_router\")\n",
        "    \n",
        "    # Supervisor Router → Genie (if talent) OR END (if not)\n",
        "    workflow.add_conditional_edges(\n",
        "        \"supervisor_router\",\n",
        "        route_after_supervisor,\n",
        "        {\n",
        "            \"genie\": \"genie\",\n",
        "            END: END\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Genie → Supervisor Summarizer (ALWAYS)\n",
        "    workflow.add_edge(\"genie\", \"supervisor_summarizer\")\n",
        "    \n",
        "    # Supervisor Summarizer → END (ALWAYS)\n",
        "    workflow.add_edge(\"supervisor_summarizer\", END)\n",
        "    \n",
        "    return workflow.compile()\n",
        "\n",
        "\n",
        "##########################################\n",
        "# Wrap LangGraph Supervisor as a ResponsesAgent with OBO\n",
        "##########################################\n",
        "\n",
        "\n",
        "class LangGraphResponsesAgent(ResponsesAgent):\n",
        "    \"\"\"\n",
        "    ResponsesAgent that creates OBO-enabled resources PER REQUEST.\n",
        "    \n",
        "    CRITICAL: OBO resources (LLM, clients, agents) are initialized in predict/predict_stream,\n",
        "    NOT in __init__, because user identity is only available at query time.\n",
        "    Uses ModelServingUserCredentials for on-behalf-of authentication.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, llm_endpoint_name: str, externally_served_agents: list):\n",
        "        \"\"\"\n",
        "        Store configuration only - NO OBO resource initialization here!\n",
        "        \n",
        "        Args:\n",
        "            llm_endpoint_name: Name of the LLM serving endpoint\n",
        "            externally_served_agents: List of agent configs (Genie, etc.)\n",
        "        \"\"\"\n",
        "        self.llm_endpoint_name = llm_endpoint_name\n",
        "        self.externally_served_agents = externally_served_agents\n",
        "        print(\"✓ LangGraphResponsesAgent initialized (config stored, OBO resources deferred)\")\n",
        "\n",
        "    def _create_graph_with_obo(self):\n",
        "        \"\"\"\n",
        "        Create graph with OBO-enabled resources.\n",
        "        \n",
        "        Called inside predict/predict_stream where user identity is available.\n",
        "        This ensures ModelServingUserCredentials() has access to the request context.\n",
        "        \n",
        "        CRITICAL: We create an OBO WorkspaceClient and pass it explicitly to the graph\n",
        "        so Genie API calls use the user's credentials for RLS enforcement.\n",
        "        \"\"\"\n",
        "        # Create OBO-enabled credentials strategy\n",
        "        obo_creds = ModelServingUserCredentials()\n",
        "        \n",
        "        # Create OBO-enabled client for UC functions\n",
        "        client = DatabricksFunctionClient(credentials_strategy=obo_creds)\n",
        "        set_uc_function_client(client)\n",
        "        \n",
        "        # Create OBO-enabled LLM\n",
        "        llm = ChatDatabricks(\n",
        "            endpoint=self.llm_endpoint_name,\n",
        "            credentials_strategy=obo_creds\n",
        "        )\n",
        "        \n",
        "        # Create OBO-enabled WorkspaceClient for Genie API calls\n",
        "        # This will be passed to the graph and used for direct Genie API calls\n",
        "        workspace_client = WorkspaceClient(credentials_strategy=obo_creds)\n",
        "        \n",
        "        # Create the graph with OBO resources\n",
        "        # The workspace_client ensures Genie queries use user credentials\n",
        "        graph = create_langgraph_with_nodes(\n",
        "            llm=llm,\n",
        "            workspace_client=workspace_client,\n",
        "            externally_served_agents=self.externally_served_agents\n",
        "        )\n",
        "        return graph\n",
        "\n",
        "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
        "        \"\"\"\n",
        "        Predict method - creates OBO graph per request.\n",
        "        \n",
        "        User identity is available here via request context.\n",
        "        \"\"\"\n",
        "        outputs = [\n",
        "            event.item\n",
        "            for event in self.predict_stream(request)\n",
        "            if event.type == \"response.output_item.done\"\n",
        "        ]\n",
        "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
        "\n",
        "    def predict_stream(\n",
        "        self,\n",
        "        request: ResponsesAgentRequest,\n",
        "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
        "        \"\"\"\n",
        "        Streaming predict - creates OBO graph per request.\n",
        "        \n",
        "        User identity is available here via request context.\n",
        "        \"\"\"\n",
        "        # Create OBO-enabled graph for THIS request with THIS user's credentials\n",
        "        agent = self._create_graph_with_obo()\n",
        "        \n",
        "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
        "        seen_ids = set()\n",
        "\n",
        "        for _, events in agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\"]):\n",
        "            node_name = tuple(events.keys())[0] if events else \"unknown\"\n",
        "            \n",
        "            print(f\"DEBUG Stream - Node: {node_name}\")\n",
        "            \n",
        "            # Get messages from this node\n",
        "            new_msgs = []\n",
        "            for v in events.values():\n",
        "                msgs_in_update = v.get(\"messages\", [])\n",
        "                print(f\"DEBUG Stream - Messages in update: {len(msgs_in_update)}\")\n",
        "                \n",
        "                for msg in msgs_in_update:\n",
        "                    if hasattr(msg, 'id') and msg.id not in seen_ids:\n",
        "                        new_msgs.append(msg)\n",
        "                        seen_ids.add(msg.id)\n",
        "                        print(f\"DEBUG Stream - Added message from {node_name}: {type(msg).__name__}\")\n",
        "            \n",
        "            # ALWAYS emit node name tag when a node executes\n",
        "            print(f\"DEBUG Stream - Emitting tag for: {node_name}\")\n",
        "            yield ResponsesAgentStreamEvent(\n",
        "                type=\"response.output_item.done\",\n",
        "                item=self.create_text_output_item(\n",
        "                    text=f\"<name>{node_name}</name>\", id=str(uuid4())\n",
        "                ),\n",
        "            )\n",
        "            \n",
        "            # Emit the actual messages if any\n",
        "            if new_msgs:\n",
        "                print(f\"DEBUG Stream - Emitting {len(new_msgs)} messages from {node_name}\")\n",
        "                yield from output_to_responses_items_stream(new_msgs)\n",
        "            else:\n",
        "                print(f\"DEBUG Stream - No new messages to emit from {node_name}\")\n",
        "\n",
        "\n",
        "#######################################################\n",
        "# Configuration (NO OBO resources initialized here!)\n",
        "#######################################################\n",
        "\n",
        "# Foundation model endpoint name (LLM initialized per-request with OBO)\n",
        "LLM_ENDPOINT_NAME = \"databricks-gpt-5-nano\"\n",
        "\n",
        "# Configure your Genie Space (agent created per-request with OBO)\n",
        "EXTERNALLY_SERVED_AGENTS = [\n",
        "    Genie(\n",
        "        space_id=\"01f0c9f705201d14b364f5daf28bb639\",  # TODO: Update with your Genie Space ID\n",
        "        name=\"talent_genie\",\n",
        "        description=\"Analyzes talent stability, mobility patterns, attrition risk, and workforce trends. Provides structured data including statistics, tables, and detailed breakdowns by department, role, tenure, and other dimensions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Optional: Add UC function-calling agents\n",
        "IN_CODE_AGENTS = []\n",
        "\n",
        "# Tools for UC function calling (if any)\n",
        "TOOLS = []\n",
        "\n",
        "print(\"✓ Agent configuration loaded (OBO resources will be created per-request)\")\n",
        "\n",
        "# Disable autolog to avoid permission issues with tracing\n",
        "# mlflow.langchain.autolog()\n",
        "\n",
        "AGENT = LangGraphResponsesAgent(LLM_ENDPOINT_NAME, EXTERNALLY_SERVED_AGENTS)\n",
        "mlflow.models.set_model(AGENT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Graph Structure\n",
        "\n",
        "Display the node and edge structure of the LangGraph.\n",
        "Le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    # For visualization, create a temporary graph without OBO\n",
        "    # (OBO is only needed at query time, not for graph structure visualization)\n",
        "    from databricks_langchain import ChatDatabricks\n",
        "    from databricks.sdk import WorkspaceClient\n",
        "    from agent import create_langgraph_with_nodes, LLM_ENDPOINT_NAME, EXTERNALLY_SERVED_AGENTS\n",
        "    \n",
        "    # Create non-OBO resources just for visualization\n",
        "    temp_llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
        "    temp_workspace_client = WorkspaceClient()  # Default credentials for visualization only\n",
        "    temp_graph = create_langgraph_with_nodes(temp_llm, temp_workspace_client, EXTERNALLY_SERVED_AGENTS)\n",
        "    \n",
        "    graph_image = temp_graph.get_graph().draw_mermaid_png()\n",
        "    display(Image(graph_image))\n",
        "    print(\"✓ Graph visualization displayed above\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not generate graph image: {e}\")\n",
        "    print(\"\\nGraph Structure (text):\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"                         START\")\n",
        "    print(\"                           ↓\")\n",
        "    print(\"                 ┌─────────────────────┐\")\n",
        "    print(\"                 │ supervisor_router   │  ← Decides: Is this talent-related?\")\n",
        "    print(\"                 └──────────┬──────────┘\")\n",
        "    print(\"                            │\")\n",
        "    print(\"              ┌─────────────┴─────────────┐\")\n",
        "    print(\"              │                           │\")\n",
        "    print(\"           TALENT                      OTHER\")\n",
        "    print(\"              │                           │\")\n",
        "    print(\"              ↓                           ↓\")\n",
        "    print(\"      ┌───────────────┐               \\\"No data\\\"\")\n",
        "    print(\"      │     genie     │                   │\")\n",
        "    print(\"      │               │                   ↓\")\n",
        "    print(\"      │ (Query data)  │                  END\")\n",
        "    print(\"      └───────┬───────┘\")\n",
        "    print(\"              ↓\")\n",
        "    print(\"      ┌───────────────────────┐\")\n",
        "    print(\"      │ supervisor_summarizer │  ← Creates 2-line summary + table\")\n",
        "    print(\"      └───────────┬───────────┘\")\n",
        "    print(\"                  ↓\")\n",
        "    print(\"                 END\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\nFlow Examples:\")\n",
        "    print(\"\\n1. TALENT Question: 'Which department has highest attrition?'\")\n",
        "    print(\"   → Router: TALENT → Genie (gets table) → Summarizer (adds summary) → User\")\n",
        "    print(\"\\n2. OTHER Question: 'What's the weather?'\")\n",
        "    print(\"   → Router: OTHER → 'I don't have that data' → User\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Agent\n",
        "\n",
        "Test the agent locally before deploying. You should see:\n",
        "1. **Summary** from the supervisor (natural language insights)\n",
        "2. **Table** from Genie (structured data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agent import AGENT\n",
        "\n",
        "# Test with a question that will require Genie to query data\n",
        "input_example = {\n",
        "    \"input\": [\n",
        "        {\"role\": \"user\", \"content\": \"Which department has the highest attrition rate?\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Get the response\n",
        "response = AGENT.predict(input_example)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test streaming to see the flow\n",
        "print(\"=\" * 80)\n",
        "print(\"STREAMING OUTPUT (shows agent handoffs and responses)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for event in AGENT.predict_stream(input_example):\n",
        "    output = event.model_dump(exclude_none=True)\n",
        "    \n",
        "    # Extract and display content\n",
        "    if 'item' in output and 'content' in output['item']:\n",
        "        for content_item in output['item']['content']:\n",
        "            if 'text' in content_item:\n",
        "                text = content_item['text']\n",
        "                \n",
        "                # Highlight agent names\n",
        "                if text.startswith('<name>'):\n",
        "                    print(f\"\\n{'='*60}\")\n",
        "                    print(f\"➜ Agent: {text}\")\n",
        "                    print(f\"{'='*60}\\n\")\n",
        "                else:\n",
        "                    print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log the Agent to MLflow\n",
        "\n",
        "Log the agent with automatic authentication for Databricks resources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from agent import EXTERNALLY_SERVED_AGENTS, LLM_ENDPOINT_NAME, TOOLS, Genie\n",
        "from databricks_langchain import UnityCatalogTool, VectorSearchRetrieverTool\n",
        "from mlflow.models.resources import (\n",
        "    DatabricksFunction,\n",
        "    DatabricksGenieSpace,\n",
        "    DatabricksServingEndpoint,\n",
        "    DatabricksSQLWarehouse,\n",
        "    DatabricksTable\n",
        ")\n",
        "from mlflow.models.auth_policy import AuthPolicy, SystemAuthPolicy, UserAuthPolicy\n",
        "from pkg_resources import get_distribution\n",
        "\n",
        "# Configure resources for SYSTEM authentication (service principal)\n",
        "# IMPORTANT: For RLS to work, do NOT add tables or Genie Space here!\n",
        "# Only add infrastructure resources that the service principal needs\n",
        "\n",
        "resources = [\n",
        "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME),  # LLM endpoint\n",
        "    DatabricksSQLWarehouse(warehouse_id=\"148ccb90800933a1\"),      # Warehouse infrastructure\n",
        "]\n",
        "\n",
        "# DO NOT add tables to system resources - tables accessed via user credentials only!\n",
        "# Tables are accessed through Genie Space using OBO user credentials\n",
        "# This ensures RLS is enforced based on the querying user's permissions\n",
        "\n",
        "# Add UC function tools if any\n",
        "for tool in TOOLS:\n",
        "    if isinstance(tool, VectorSearchRetrieverTool):\n",
        "        resources.extend(tool.resources)\n",
        "    elif isinstance(tool, UnityCatalogTool):\n",
        "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
        "\n",
        "# Add other served agents (but NOT Genie Space!)\n",
        "for agent in EXTERNALLY_SERVED_AGENTS:\n",
        "    if isinstance(agent, Genie):\n",
        "        # DO NOT add Genie Space to system resources!\n",
        "        # Genie must be accessed ONLY via user credentials for RLS to work\n",
        "        # The service principal should NOT have access to Genie Space\n",
        "        pass\n",
        "    else:\n",
        "        resources.append(DatabricksServingEndpoint(endpoint_name=agent.endpoint_name))\n",
        "\n",
        "# Configure OBO authentication policies\n",
        "# System auth policy: Agent authenticates to these resources automatically\n",
        "# NOTE: Genie Space is intentionally NOT in system resources - it's user-only!\n",
        "systemAuthPolicy = SystemAuthPolicy(resources=resources)\n",
        "\n",
        "# User auth policy: Define API scopes for on-behalf-of user authentication\n",
        "userAuthPolicy = UserAuthPolicy(\n",
        "    api_scopes=[\n",
        "        \"serving.serving-endpoints\",     # For LLM endpoint access\n",
        "        \"sql.warehouses\",                # For SQL warehouse access\n",
        "        \"sql.statement-execution\",       # For executing SQL queries on tables\n",
        "        \"dashboards.genie\",              # For Genie Space access (CRITICAL for OBO)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Log the model with OBO authentication\n",
        "# Note: Don't pass resources separately - they're already in SystemAuthPolicy\n",
        "with mlflow.start_run():\n",
        "    logged_agent_info = mlflow.pyfunc.log_model(\n",
        "        name=\"agent\",\n",
        "        python_model=\"agent.py\",\n",
        "        auth_policy=AuthPolicy(\n",
        "            system_auth_policy=systemAuthPolicy,\n",
        "            user_auth_policy=userAuthPolicy\n",
        "        ),\n",
        "        pip_requirements=[\n",
        "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
        "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
        "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
        "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
        "            f\"langgraph-supervisor=={get_distribution('langgraph-supervisor').version}\",\n",
        "            \"databricks-ai-bridge\",  # Required for OBO authentication\n",
        "        ],\n",
        "    )\n",
        "\n",
        "print(f\"✅ Model logged successfully with OBO authentication!\")\n",
        "print(f\"Run ID: {logged_agent_info.run_id}\")\n",
        "print(f\"Model URI: {logged_agent_info.model_uri}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register to Unity Catalog\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.set_registry_uri(\"databricks-uc\")\n",
        "\n",
        "# TODO: Update these with your catalog, schema, and model name\n",
        "catalog = \"akash_s_demo\"\n",
        "schema = \"talent\"\n",
        "model_name = \"talent_agent_v1\"\n",
        "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
        "\n",
        "# Register the model\n",
        "uc_registered_model_info = mlflow.register_model(\n",
        "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
        ")\n",
        "\n",
        "print(f\"✅ Model registered to Unity Catalog!\")\n",
        "print(f\"Model: {UC_MODEL_NAME}\")\n",
        "print(f\"Version: {uc_registered_model_info.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the Agent\n",
        "\n",
        "Deploy the agent to a serving endpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from databricks import agents\n",
        "\n",
        "# Deploy the agent\n",
        "deployment_info = agents.deploy(\n",
        "    UC_MODEL_NAME, \n",
        "    uc_registered_model_info.version,\n",
        "    tags={\"enhanced\": \"with_summary\"},\n",
        "    deploy_feedback_model=False\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🚀 DEPLOYMENT INITIATED\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nYour agent with enhanced summarization is being deployed!\")\n",
        "print(\"\\n📊 What to expect:\")\n",
        "print(\"  • Natural language summaries from Llama 3.1\")\n",
        "print(\"  • Structured tables from Genie\")\n",
        "print(\"  • Both in a single response\")\n",
        "print(\"\\nThis deployment can take up to 15 minutes.\")\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Output\n",
        "\n",
        "### Question: \"Give me attrition rates for each BU\"\n",
        "\n",
        "**What you'll get:**\n",
        "\n",
        "```\n",
        "Sales department has the highest attrition rate at 15.2%, significantly above the 8.1% company average.\n",
        "Engineering maintains the strongest retention at 6.3%, indicating effective retention programs in technical roles.\n",
        "\n",
        "| Department  | Attrition Rate | Employee Count | Avg Tenure |\n",
        "|-------------|----------------|----------------|------------|\n",
        "| Sales       | 15.2%          | 450            | 2.3 years  |\n",
        "| Support     | 12.8%          | 320            | 2.8 years  |\n",
        "| Marketing   | 10.5%          | 180            | 3.2 years  |\n",
        "| Operations  | 9.2%           | 280            | 3.8 years  |\n",
        "| Engineering | 6.3%           | 520            | 4.5 years  |\n",
        "```\n",
        "\n",
        "**In your Dash app, this will display as:**\n",
        "- ✅ **2-line summary** at the top (easy to read)\n",
        "- ✅ **Formatted table** below (with proper styling)\n",
        "- ✅ **Agent badge** showing which agent answered\n",
        "\n",
        "## Key Features\n",
        "\n",
        "✅ **Concise** - Exactly 2 lines of summary, no fluff  \n",
        "✅ **Specific** - Uses actual numbers from the data  \n",
        "✅ **Complete** - Full table preserved for detailed analysis  \n",
        "✅ **Frontend Ready** - Dash app already parses and displays this format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent System with Genie + LLM Summarization\n",
        "\n",
        "This notebook creates a multi-agent system where:\n",
        "1. **Genie Agent** provides structured data (tables, statistics)\n",
        "2. **Supervisor Agent** (Llama 3.1) creates natural language summaries\n",
        "3. **Output includes BOTH** the table and the summary\n",
        "\n",
        "## Prerequisites\n",
        "- Genie Space created and configured\n",
        "- Databricks serving endpoint access\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U -qqq langgraph-supervisor==0.0.30 mlflow[databricks] databricks-langchain databricks-agents databricks-ai-bridge uv \n",
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Multi-Agent System with Intelligent Routing\n",
        "\n",
        "### Graph Architecture (Nodes, Edges, and Conditional Routing):\n",
        "\n",
        "```\n",
        "                    START\n",
        "                      ‚Üì\n",
        "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "            ‚îÇSupervisor Router ‚îÇ  ‚Üê Classifies question\n",
        "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                     ‚îÇ\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ                         ‚îÇ\n",
        "     TALENT                    OTHER\n",
        "        ‚îÇ                         ‚îÇ\n",
        "        ‚Üì                         ‚Üì\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            \"No data available\"\n",
        "  ‚îÇ  Genie   ‚îÇ                   ‚îÇ\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚Üì\n",
        "       ‚îÇ                        END\n",
        "       ‚Üì\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇSupervisor       ‚îÇ  ‚Üê Creates summary + table\n",
        "‚îÇSummarizer       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "         ‚Üì\n",
        "        END\n",
        "```\n",
        "\n",
        "### Key Features:\n",
        "- ‚úÖ **Intelligent routing**: Supervisor decides if question is talent-related\n",
        "- ‚úÖ **Conditional logic**: Only calls Genie for talent questions\n",
        "- ‚úÖ **Efficient**: Avoids unnecessary API calls for off-topic questions  \n",
        "- ‚úÖ **Guaranteed format**: All Genie responses get 2-line summary + table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile agent.py\n",
        "import json\n",
        "from typing import Generator, Literal\n",
        "from uuid import uuid4\n",
        "\n",
        "import mlflow\n",
        "from databricks_langchain import (\n",
        "    ChatDatabricks,\n",
        "    DatabricksFunctionClient,\n",
        "    UCFunctionToolkit,\n",
        "    set_uc_function_client,\n",
        ")\n",
        "from databricks_langchain.genie import GenieAgent\n",
        "from databricks_ai_bridge import ModelServingUserCredentials  # OBO authentication\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain.agents import create_agent\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "from langgraph_supervisor import create_supervisor\n",
        "from mlflow.pyfunc import ResponsesAgent\n",
        "from mlflow.types.responses import (\n",
        "    ResponsesAgentRequest,\n",
        "    ResponsesAgentResponse,\n",
        "    ResponsesAgentStreamEvent,\n",
        "    output_to_responses_items_stream,\n",
        "    to_chat_completions_input,\n",
        ")\n",
        "from pydantic import BaseModel\n",
        "\n",
        "########################################\n",
        "# Agent Configuration Models\n",
        "########################################\n",
        "\n",
        "GENIE = \"genie\"\n",
        "\n",
        "\n",
        "class ServedSubAgent(BaseModel):\n",
        "    endpoint_name: str\n",
        "    name: str\n",
        "    task: Literal[\"agent/v1/responses\", \"agent/v1/chat\", \"agent/v2/chat\"]\n",
        "    description: str\n",
        "\n",
        "\n",
        "class Genie(BaseModel):\n",
        "    space_id: str\n",
        "    name: str\n",
        "    task: str = GENIE\n",
        "    description: str\n",
        "\n",
        "\n",
        "class InCodeSubAgent(BaseModel):\n",
        "    tools: list[str]\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "\n",
        "def stringify_content(state):\n",
        "    \"\"\"Convert content to string format for processing\"\"\"\n",
        "    msgs = state[\"messages\"]\n",
        "    if isinstance(msgs[-1].content, list):\n",
        "        msgs[-1].content = json.dumps(msgs[-1].content, indent=4)\n",
        "    return {\"messages\": msgs}\n",
        "\n",
        "\n",
        "########################################\n",
        "# Create Custom LangGraph with Explicit Nodes and Edges\n",
        "########################################\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "\n",
        "\n",
        "# Define the state structure\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, operator.add]\n",
        "    next_step: str  # Track routing decision\n",
        "\n",
        "\n",
        "def create_langgraph_with_nodes(\n",
        "    llm: Runnable,\n",
        "    externally_served_agents: list[ServedSubAgent] = [],\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a LangGraph with intelligent routing:\n",
        "    - User Question ‚Üí Supervisor Router (decides if talent-related)\n",
        "    - If YES ‚Üí Genie Node (gets data) ‚Üí Supervisor Summarizer (creates summary)\n",
        "    - If NO ‚Üí Direct response (no data available)\n",
        "    - Final Response ‚Üí END (returns to user)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create Genie agent\n",
        "    genie_agent = None\n",
        "    for agent in externally_served_agents:\n",
        "        if isinstance(agent, Genie):\n",
        "            genie_agent = GenieAgent(\n",
        "                genie_space_id=agent.space_id,\n",
        "                genie_agent_name=agent.name,\n",
        "                description=agent.description,\n",
        "            )\n",
        "            genie_agent.name = agent.name\n",
        "            break\n",
        "    \n",
        "    if not genie_agent:\n",
        "        raise ValueError(\"Genie agent is required\")\n",
        "    \n",
        "    # Define Supervisor Router node (decides if talent-related)\n",
        "    def supervisor_router(state: AgentState):\n",
        "        \"\"\"Supervisor router - determines if question is about talent/workforce data\"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        \n",
        "        # Get the user's question\n",
        "        user_question = \"\"\n",
        "        for msg in messages:\n",
        "            if hasattr(msg, 'content') and msg.content:\n",
        "                if isinstance(msg, HumanMessage) or (hasattr(msg, 'role') and msg.role == 'user'):\n",
        "                    user_question = msg.content\n",
        "                    break\n",
        "        \n",
        "        print(f\"DEBUG Router - User question: {user_question[:100]}\")\n",
        "        \n",
        "        # Use LLM with few-shot examples for robust classification\n",
        "        routing_prompt = f\"\"\"You are a routing assistant for a TALENT & WORKFORCE ANALYTICS chatbot.\n",
        "\n",
        "This chatbot can ONLY answer questions about:\n",
        "‚úì Workforce data (employees, headcount, demographics)\n",
        "‚úì Organizational structure (departments, business units, teams, managers)  \n",
        "‚úì Attrition & retention (turnover, exits, resignations, churn)\n",
        "‚úì Employee mobility (promotions, transfers, career paths)\n",
        "‚úì HR metrics (compensation, performance, tenure, reviews)\n",
        "‚úì Workforce trends and analytics\n",
        "\n",
        "The chatbot CANNOT answer questions about:\n",
        "‚úó General knowledge, facts, or trivia\n",
        "‚úó Current events, news, or weather\n",
        "‚úó Products, services, or customer data (unless about employees)\n",
        "‚úó Technical support or IT issues\n",
        "‚úó Anything unrelated to employees/workforce\n",
        "\n",
        "TASK: Classify if the following question can be answered with workforce/talent data.\n",
        "\n",
        "Question: \"{user_question}\"\n",
        "\n",
        "CLASSIFICATION EXAMPLES (learn the pattern):\n",
        "\n",
        "\"Which department has highest attrition rate?\" ‚Üí TALENT (clearly about workforce)\n",
        "\"Give me BU level attrition details\" ‚Üí TALENT (BU = business unit, workforce metric)\n",
        "\"Show me employee turnover\" ‚Üí TALENT (employee metric)\n",
        "\"Attrition by team\" ‚Üí TALENT (workforce analytics)\n",
        "\"What percentage of people left?\" ‚Üí TALENT (people = employees)  \n",
        "\"Details about organizational structure\" ‚Üí TALENT (org data)\n",
        "\"How many staff in each division?\" ‚Üí TALENT (headcount query)\n",
        "\"Show me the data\" ‚Üí TALENT (assume workforce data in this context)\n",
        "\"Give me details\" ‚Üí TALENT (assume talent details in this context)\n",
        "\"What are the rates?\" ‚Üí TALENT (likely workforce rates)\n",
        "\"What's the weather today?\" ‚Üí OTHER (unrelated to workforce)\n",
        "\"Tell me about Python programming\" ‚Üí OTHER (technical, not workforce)\n",
        "\"What is a good restaurant?\" ‚Üí OTHER (unrelated to talent)\n",
        "\n",
        "DECISION RULE:\n",
        "- If the question could plausibly be asking for workforce/talent/organizational data ‚Üí TALENT\n",
        "- If the question is clearly about a non-workforce topic ‚Üí OTHER\n",
        "- When in doubt, choose TALENT (better to try and fail than miss valid questions)\n",
        "\n",
        "Your classification (respond with ONLY the word \"TALENT\" or \"OTHER\"):\"\"\"\n",
        "        \n",
        "        routing_response = llm.invoke([HumanMessage(content=routing_prompt)])\n",
        "        decision = routing_response.content.strip().upper()\n",
        "        \n",
        "        print(f\"DEBUG Router - Decision: {decision}\")\n",
        "        \n",
        "        if \"TALENT\" in decision:\n",
        "            # Route to Genie\n",
        "            return {\"next_step\": \"genie\", \"messages\": []}\n",
        "        else:\n",
        "            # Return message saying we don't have data\n",
        "            response_msg = AIMessage(\n",
        "                content=\"I'm specialized in talent and workforce analytics. I don't have information about that topic. Please ask me questions about attrition, employee mobility, retention, or workforce trends.\",\n",
        "                name=\"supervisor\"\n",
        "            )\n",
        "            return {\"next_step\": \"end\", \"messages\": [response_msg]}\n",
        "    \n",
        "    # Define Genie node function\n",
        "    def genie_node(state: AgentState):\n",
        "        \"\"\"Genie node - queries data and returns structured results\"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        \n",
        "        print(f\"DEBUG Genie - Input messages: {len(messages)}\")\n",
        "        \n",
        "        # Invoke Genie\n",
        "        response = genie_agent.invoke({\"messages\": messages})\n",
        "        \n",
        "        print(f\"DEBUG Genie - Response type: {type(response)}\")\n",
        "        print(f\"DEBUG Genie - Response keys: {response.keys() if isinstance(response, dict) else 'Not a dict'}\")\n",
        "        \n",
        "        # GenieAgent returns the response differently - check for 'output' or last message\n",
        "        genie_output = None\n",
        "        \n",
        "        # Try to get the actual Genie response\n",
        "        if isinstance(response, dict):\n",
        "            # Check if there's an 'output' field (common in agent responses)\n",
        "            if 'output' in response:\n",
        "                genie_output = response['output']\n",
        "                print(f\"DEBUG Genie - Found 'output' field: {str(genie_output)[:200]}\")\n",
        "            # Check if messages were appended\n",
        "            elif 'messages' in response and len(response['messages']) > len(messages):\n",
        "                new_msgs = response['messages'][len(messages):]\n",
        "                genie_output = new_msgs[-1] if new_msgs else None\n",
        "                print(f\"DEBUG Genie - Found new messages: {len(new_msgs)}\")\n",
        "            # Otherwise, get the last message which should have Genie's response\n",
        "            elif 'messages' in response and response['messages']:\n",
        "                last_msg = response['messages'][-1]\n",
        "                genie_output = last_msg\n",
        "                print(f\"DEBUG Genie - Using last message: {type(last_msg)}\")\n",
        "        \n",
        "        # Convert to AIMessage if needed\n",
        "        if genie_output:\n",
        "            if isinstance(genie_output, str):\n",
        "                genie_message = AIMessage(content=genie_output, name=\"genie\")\n",
        "            elif hasattr(genie_output, 'content'):\n",
        "                genie_message = AIMessage(content=genie_output.content, name=\"genie\")\n",
        "            elif isinstance(genie_output, dict) and 'content' in genie_output:\n",
        "                genie_message = AIMessage(content=genie_output['content'], name=\"genie\")\n",
        "            else:\n",
        "                genie_message = AIMessage(content=str(genie_output), name=\"genie\")\n",
        "            \n",
        "            print(f\"DEBUG Genie - Created AIMessage with content length: {len(genie_message.content)}\")\n",
        "            return {\"messages\": [genie_message]}\n",
        "        else:\n",
        "            print(\"ERROR Genie - No output found!\")\n",
        "            return {\"messages\": [AIMessage(content=\"Genie returned no data.\", name=\"genie\")]}\n",
        "    \n",
        "    # Helper function to clean pandas-formatted markdown tables\n",
        "    def clean_pandas_table(text):\n",
        "        \"\"\"\n",
        "        Remove pandas index column from markdown tables.\n",
        "        Converts: |    | col1 | col2 |  ‚Üí  | col1 | col2 |\n",
        "                  |---:|:-----|------|      |:-----|------|\n",
        "                  |  0 | val1 | val2 |      | val1 | val2 |\n",
        "        \"\"\"\n",
        "        import re\n",
        "        \n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "        \n",
        "        for line in lines:\n",
        "            if '|' in line:\n",
        "                # Split by pipe and strip whitespace\n",
        "                cells = [cell.strip() for cell in line.split('|')]\n",
        "                \n",
        "                # Check if this is a table line (has multiple cells)\n",
        "                if len(cells) >= 3:  # At least: ['', 'content', '']\n",
        "                    # Remove leading/trailing empty cells\n",
        "                    while cells and cells[0] == '':\n",
        "                        cells.pop(0)\n",
        "                    while cells and cells[-1] == '':\n",
        "                        cells.pop()\n",
        "                    \n",
        "                    # Check if this is a separator line (only dashes, colons, spaces)\n",
        "                    is_separator = cells and all(re.match(r'^[-:\\s]+$', cell) for cell in cells)\n",
        "                    \n",
        "                    # Check if first cell is pandas index (empty, numeric, or separator marker)\n",
        "                    if cells and (cells[0] == '' or \n",
        "                                  cells[0].isdigit() or \n",
        "                                  re.match(r'^\\s*\\d+\\s*$', cells[0]) or\n",
        "                                  (is_separator and re.match(r'^-+:?$', cells[0]))):\n",
        "                        # Remove the first cell (pandas index)\n",
        "                        cells = cells[1:]\n",
        "                    \n",
        "                    # Rebuild the line with clean cells\n",
        "                    if cells:\n",
        "                        cleaned_line = '| ' + ' | '.join(cells) + ' |'\n",
        "                        cleaned_lines.append(cleaned_line)\n",
        "                else:\n",
        "                    cleaned_lines.append(line)\n",
        "            else:\n",
        "                cleaned_lines.append(line)\n",
        "        \n",
        "        return '\\n'.join(cleaned_lines)\n",
        "    \n",
        "    # Define Supervisor Summarizer node (creates summary after Genie)\n",
        "    def supervisor_summarizer(state: AgentState):\n",
        "        \"\"\"Supervisor summarizer - creates 2-line summary + preserves Genie's table\"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        \n",
        "        # Get ALL messages - find the one from Genie (should be AI message after user message)\n",
        "        genie_response = \"\"\n",
        "        \n",
        "        # Look for the last AI message (from Genie)\n",
        "        for msg in reversed(messages):\n",
        "            if hasattr(msg, 'content') and msg.content:\n",
        "                # Check if it's an AI message and has actual content\n",
        "                if isinstance(msg, AIMessage) or (hasattr(msg, 'type') and msg.type == 'ai'):\n",
        "                    content = str(msg.content)\n",
        "                    # Skip if it's too short or empty\n",
        "                    if content and len(content.strip()) > 10:\n",
        "                        genie_response = content\n",
        "                        break\n",
        "        \n",
        "        # Debug: Print what we got from Genie\n",
        "        print(f\"DEBUG - Messages count: {len(messages)}\")\n",
        "        print(f\"DEBUG - Genie response length: {len(genie_response) if genie_response else 0}\")\n",
        "        if genie_response:\n",
        "            print(f\"DEBUG - Genie response preview: {genie_response[:200]}...\")\n",
        "        \n",
        "        if not genie_response or len(genie_response.strip()) < 10:\n",
        "            # If still no response, get the full state for debugging\n",
        "            error_msg = f\"No data received from Genie. Messages in state: {len(messages)}\"\n",
        "            print(f\"ERROR: {error_msg}\")\n",
        "            for i, msg in enumerate(messages):\n",
        "                print(f\"  Message {i}: type={type(msg).__name__}, has_content={hasattr(msg, 'content')}\")\n",
        "                if hasattr(msg, 'content'):\n",
        "                    content_preview = str(msg.content)[:100]\n",
        "                    print(f\"    Content preview: {content_preview}\")\n",
        "            return {\"messages\": [AIMessage(content=error_msg)]}\n",
        "        \n",
        "        # Clean up pandas-formatted tables (remove index column)\n",
        "        genie_response = clean_pandas_table(genie_response)\n",
        "        print(f\"DEBUG - Cleaned genie response length: {len(genie_response)}\")\n",
        "        print(f\"DEBUG - Cleaned genie response preview: {genie_response[:200]}...\")\n",
        "        \n",
        "        # Create supervisor prompt - be VERY explicit\n",
        "        system_prompt = \"\"\"You are a data analyst. Your job is to write a 2-line summary and include the original table.\n",
        "\n",
        "OUTPUT FORMAT (copy exactly):\n",
        "[Line 1: Key finding with number]\n",
        "[Line 2: Second insight]\n",
        "\n",
        "[PASTE THE ORIGINAL TABLE HERE]\n",
        "\n",
        "EXAMPLE:\n",
        "Sales has highest attrition at 15.2%, above the 8.1% average.\n",
        "Engineering shows best retention at 6.3% with effective programs.\n",
        "\n",
        "| Department | Rate  | Count |\n",
        "|------------|-------|-------|\n",
        "| Sales      | 15.2% | 450   |\n",
        "| Engineering| 6.3%  | 520   |\n",
        "\n",
        "RULES:\n",
        "- Write EXACTLY 2 short lines analyzing the data\n",
        "- Add blank line\n",
        "- Copy the COMPLETE original table unchanged\n",
        "- That's it - nothing else\"\"\"\n",
        "        \n",
        "        # Create messages for LLM with explicit instruction\n",
        "        user_prompt = f\"\"\"Here is the data with a table:\n",
        "\n",
        "{genie_response}\n",
        "\n",
        "Instructions:\n",
        "1. Write 2 lines summarizing the key findings\n",
        "2. Include the complete table from above\n",
        "\n",
        "Your response:\"\"\"\n",
        "        \n",
        "        supervisor_messages = [\n",
        "            SystemMessage(content=system_prompt),\n",
        "            HumanMessage(content=user_prompt)\n",
        "        ]\n",
        "        \n",
        "        # Get summary from LLM\n",
        "        summary_response = llm.invoke(supervisor_messages)\n",
        "        \n",
        "        # Combine summary with original table to ensure table is preserved\n",
        "        final_response = summary_response.content\n",
        "        \n",
        "        # If the table isn't in the response, append it\n",
        "        if '|' not in final_response and '|' in genie_response:\n",
        "            print(\"DEBUG - Table not in LLM response, appending original table\")\n",
        "            final_response = f\"{final_response}\\n\\n{genie_response}\"\n",
        "        \n",
        "        print(f\"DEBUG Supervisor - Final response length: {len(final_response)}\")\n",
        "        print(f\"DEBUG Supervisor - Response preview: {final_response[:300]}\")\n",
        "        \n",
        "        # Create a message with explicit ID to ensure it's unique\n",
        "        summary_message = AIMessage(\n",
        "            content=final_response,\n",
        "            name=\"supervisor_summarizer\",\n",
        "            id=str(uuid4())  # Ensure unique ID\n",
        "        )\n",
        "        \n",
        "        print(f\"DEBUG Supervisor - Created message with ID: {summary_message.id}\")\n",
        "        \n",
        "        # Return as a single message with the summary content\n",
        "        return {\"messages\": [summary_message]}\n",
        "    \n",
        "    # Conditional edge function\n",
        "    def route_after_supervisor(state: AgentState):\n",
        "        \"\"\"Route based on supervisor's decision\"\"\"\n",
        "        next_step = state.get(\"next_step\", \"end\")\n",
        "        print(f\"DEBUG Routing - Next step: {next_step}\")\n",
        "        \n",
        "        if next_step == \"genie\":\n",
        "            return \"genie\"\n",
        "        else:\n",
        "            return END\n",
        "    \n",
        "    # Build the graph with conditional routing\n",
        "    workflow = StateGraph(AgentState)\n",
        "    \n",
        "    # Add nodes\n",
        "    workflow.add_node(\"supervisor_router\", supervisor_router)\n",
        "    workflow.add_node(\"genie\", genie_node)\n",
        "    workflow.add_node(\"supervisor_summarizer\", supervisor_summarizer)\n",
        "    \n",
        "    # Define edges with conditional routing\n",
        "    # START ‚Üí Supervisor Router (decides if talent-related)\n",
        "    workflow.set_entry_point(\"supervisor_router\")\n",
        "    \n",
        "    # Supervisor Router ‚Üí Genie (if talent) OR END (if not)\n",
        "    workflow.add_conditional_edges(\n",
        "        \"supervisor_router\",\n",
        "        route_after_supervisor,\n",
        "        {\n",
        "            \"genie\": \"genie\",\n",
        "            END: END\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Genie ‚Üí Supervisor Summarizer (ALWAYS)\n",
        "    workflow.add_edge(\"genie\", \"supervisor_summarizer\")\n",
        "    \n",
        "    # Supervisor Summarizer ‚Üí END (ALWAYS)\n",
        "    workflow.add_edge(\"supervisor_summarizer\", END)\n",
        "    \n",
        "    return workflow.compile()\n",
        "\n",
        "\n",
        "##########################################\n",
        "# Wrap LangGraph Supervisor as a ResponsesAgent with OBO\n",
        "##########################################\n",
        "\n",
        "\n",
        "class LangGraphResponsesAgent(ResponsesAgent):\n",
        "    \"\"\"\n",
        "    ResponsesAgent that creates OBO-enabled resources PER REQUEST.\n",
        "    \n",
        "    CRITICAL: OBO resources (LLM, clients, agents) are initialized in predict/predict_stream,\n",
        "    NOT in __init__, because user identity is only available at query time.\n",
        "    Uses ModelServingUserCredentials for on-behalf-of authentication.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, llm_endpoint_name: str, externally_served_agents: list):\n",
        "        \"\"\"\n",
        "        Store configuration only - NO OBO resource initialization here!\n",
        "        \n",
        "        Args:\n",
        "            llm_endpoint_name: Name of the LLM serving endpoint\n",
        "            externally_served_agents: List of agent configs (Genie, etc.)\n",
        "        \"\"\"\n",
        "        self.llm_endpoint_name = llm_endpoint_name\n",
        "        self.externally_served_agents = externally_served_agents\n",
        "        print(\"‚úì LangGraphResponsesAgent initialized (config stored, OBO resources deferred)\")\n",
        "\n",
        "    def _create_graph_with_obo(self):\n",
        "        \"\"\"\n",
        "        Create graph with OBO-enabled resources.\n",
        "        \n",
        "        Called inside predict/predict_stream where user identity is available.\n",
        "        This ensures ModelServingUserCredentials() has access to the request context.\n",
        "        \"\"\"\n",
        "        # Create OBO-enabled client for UC functions and Genie\n",
        "        client = DatabricksFunctionClient(credentials_provider=ModelServingUserCredentials())\n",
        "        set_uc_function_client(client)\n",
        "        \n",
        "        # Create OBO-enabled LLM\n",
        "        llm = ChatDatabricks(\n",
        "            endpoint=self.llm_endpoint_name,\n",
        "            credentials_provider=ModelServingUserCredentials()\n",
        "        )\n",
        "        \n",
        "        # Create the graph with OBO resources\n",
        "        graph = create_langgraph_with_nodes(llm, self.externally_served_agents)\n",
        "        \n",
        "        return graph\n",
        "\n",
        "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
        "        \"\"\"\n",
        "        Predict method - creates OBO graph per request.\n",
        "        \n",
        "        User identity is available here via request context.\n",
        "        \"\"\"\n",
        "        outputs = [\n",
        "            event.item\n",
        "            for event in self.predict_stream(request)\n",
        "            if event.type == \"response.output_item.done\"\n",
        "        ]\n",
        "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
        "\n",
        "    def predict_stream(\n",
        "        self,\n",
        "        request: ResponsesAgentRequest,\n",
        "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
        "        \"\"\"\n",
        "        Streaming predict - creates OBO graph per request.\n",
        "        \n",
        "        User identity is available here via request context.\n",
        "        \"\"\"\n",
        "        # Create OBO-enabled graph for THIS request with THIS user's credentials\n",
        "        agent = self._create_graph_with_obo()\n",
        "        \n",
        "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
        "        seen_ids = set()\n",
        "\n",
        "        for _, events in agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\"]):\n",
        "            node_name = tuple(events.keys())[0] if events else \"unknown\"\n",
        "            \n",
        "            print(f\"DEBUG Stream - Node: {node_name}\")\n",
        "            \n",
        "            # Get messages from this node\n",
        "            new_msgs = []\n",
        "            for v in events.values():\n",
        "                msgs_in_update = v.get(\"messages\", [])\n",
        "                print(f\"DEBUG Stream - Messages in update: {len(msgs_in_update)}\")\n",
        "                \n",
        "                for msg in msgs_in_update:\n",
        "                    if hasattr(msg, 'id') and msg.id not in seen_ids:\n",
        "                        new_msgs.append(msg)\n",
        "                        seen_ids.add(msg.id)\n",
        "                        print(f\"DEBUG Stream - Added message from {node_name}: {type(msg).__name__}\")\n",
        "            \n",
        "            # ALWAYS emit node name tag when a node executes\n",
        "            print(f\"DEBUG Stream - Emitting tag for: {node_name}\")\n",
        "            yield ResponsesAgentStreamEvent(\n",
        "                type=\"response.output_item.done\",\n",
        "                item=self.create_text_output_item(\n",
        "                    text=f\"<name>{node_name}</name>\", id=str(uuid4())\n",
        "                ),\n",
        "            )\n",
        "            \n",
        "            # Emit the actual messages if any\n",
        "            if new_msgs:\n",
        "                print(f\"DEBUG Stream - Emitting {len(new_msgs)} messages from {node_name}\")\n",
        "                yield from output_to_responses_items_stream(new_msgs)\n",
        "            else:\n",
        "                print(f\"DEBUG Stream - No new messages to emit from {node_name}\")\n",
        "\n",
        "\n",
        "#######################################################\n",
        "# Configuration (NO OBO resources initialized here!)\n",
        "#######################################################\n",
        "\n",
        "# Foundation model endpoint name (LLM initialized per-request with OBO)\n",
        "LLM_ENDPOINT_NAME = \"databricks-gpt-5-nano\"\n",
        "\n",
        "# Configure your Genie Space (agent created per-request with OBO)\n",
        "EXTERNALLY_SERVED_AGENTS = [\n",
        "    Genie(\n",
        "        space_id=\"01f0c9f705201d14b364f5daf28bb639\",  # TODO: Update with your Genie Space ID\n",
        "        name=\"talent_genie\",\n",
        "        description=\"Analyzes talent stability, mobility patterns, attrition risk, and workforce trends. Provides structured data including statistics, tables, and detailed breakdowns by department, role, tenure, and other dimensions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Optional: Add UC function-calling agents\n",
        "IN_CODE_AGENTS = []\n",
        "\n",
        "# Tools for UC function calling (if any)\n",
        "TOOLS = []\n",
        "\n",
        "print(\"‚úì Agent configuration loaded (OBO resources will be created per-request)\")\n",
        "\n",
        "# Disable autolog to avoid permission issues with tracing\n",
        "# mlflow.langchain.autolog()\n",
        "\n",
        "AGENT = LangGraphResponsesAgent(LLM_ENDPOINT_NAME, EXTERNALLY_SERVED_AGENTS)\n",
        "mlflow.models.set_model(AGENT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Graph Structure\n",
        "\n",
        "Display the node and edge structure of the LangGraph.\n",
        "Le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    # For visualization, create a temporary graph without OBO\n",
        "    # (OBO is only needed at query time, not for graph structure visualization)\n",
        "    from databricks_langchain import ChatDatabricks\n",
        "    from agent import create_langgraph_with_nodes, LLM_ENDPOINT_NAME, EXTERNALLY_SERVED_AGENTS\n",
        "    \n",
        "    # Create non-OBO LLM just for visualization\n",
        "    temp_llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
        "    temp_graph = create_langgraph_with_nodes(temp_llm, EXTERNALLY_SERVED_AGENTS)\n",
        "    \n",
        "    graph_image = temp_graph.get_graph().draw_mermaid_png()\n",
        "    display(Image(graph_image))\n",
        "    print(\"‚úì Graph visualization displayed above\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not generate graph image: {e}\")\n",
        "    print(\"\\nGraph Structure (text):\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"                         START\")\n",
        "    print(\"                           ‚Üì\")\n",
        "    print(\"                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
        "    print(\"                 ‚îÇ supervisor_router   ‚îÇ  ‚Üê Decides: Is this talent-related?\")\n",
        "    print(\"                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
        "    print(\"                            ‚îÇ\")\n",
        "    print(\"              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
        "    print(\"              ‚îÇ                           ‚îÇ\")\n",
        "    print(\"           TALENT                      OTHER\")\n",
        "    print(\"              ‚îÇ                           ‚îÇ\")\n",
        "    print(\"              ‚Üì                           ‚Üì\")\n",
        "    print(\"      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               \\\"No data\\\"\")\n",
        "    print(\"      ‚îÇ     genie     ‚îÇ                   ‚îÇ\")\n",
        "    print(\"      ‚îÇ               ‚îÇ                   ‚Üì\")\n",
        "    print(\"      ‚îÇ (Query data)  ‚îÇ                  END\")\n",
        "    print(\"      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
        "    print(\"              ‚Üì\")\n",
        "    print(\"      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
        "    print(\"      ‚îÇ supervisor_summarizer ‚îÇ  ‚Üê Creates 2-line summary + table\")\n",
        "    print(\"      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
        "    print(\"                  ‚Üì\")\n",
        "    print(\"                 END\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\nFlow Examples:\")\n",
        "    print(\"\\n1. TALENT Question: 'Which department has highest attrition?'\")\n",
        "    print(\"   ‚Üí Router: TALENT ‚Üí Genie (gets table) ‚Üí Summarizer (adds summary) ‚Üí User\")\n",
        "    print(\"\\n2. OTHER Question: 'What's the weather?'\")\n",
        "    print(\"   ‚Üí Router: OTHER ‚Üí 'I don't have that data' ‚Üí User\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Agent\n",
        "\n",
        "Test the agent locally before deploying. You should see:\n",
        "1. **Summary** from the supervisor (natural language insights)\n",
        "2. **Table** from Genie (structured data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agent import AGENT\n",
        "\n",
        "# Test with a question that will require Genie to query data\n",
        "input_example = {\n",
        "    \"input\": [\n",
        "        {\"role\": \"user\", \"content\": \"Which department has the highest attrition rate?\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Get the response\n",
        "response = AGENT.predict(input_example)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test streaming to see the flow\n",
        "print(\"=\" * 80)\n",
        "print(\"STREAMING OUTPUT (shows agent handoffs and responses)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for event in AGENT.predict_stream(input_example):\n",
        "    output = event.model_dump(exclude_none=True)\n",
        "    \n",
        "    # Extract and display content\n",
        "    if 'item' in output and 'content' in output['item']:\n",
        "        for content_item in output['item']['content']:\n",
        "            if 'text' in content_item:\n",
        "                text = content_item['text']\n",
        "                \n",
        "                # Highlight agent names\n",
        "                if text.startswith('<name>'):\n",
        "                    print(f\"\\n{'='*60}\")\n",
        "                    print(f\"‚ûú Agent: {text}\")\n",
        "                    print(f\"{'='*60}\\n\")\n",
        "                else:\n",
        "                    print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log the Agent to MLflow\n",
        "\n",
        "Log the agent with automatic authentication for Databricks resources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from agent import EXTERNALLY_SERVED_AGENTS, LLM_ENDPOINT_NAME, TOOLS, Genie\n",
        "from databricks_langchain import UnityCatalogTool, VectorSearchRetrieverTool\n",
        "from mlflow.models.resources import (\n",
        "    DatabricksFunction,\n",
        "    DatabricksGenieSpace,\n",
        "    DatabricksServingEndpoint,\n",
        "    DatabricksSQLWarehouse,\n",
        "    DatabricksTable\n",
        ")\n",
        "from mlflow.models.auth_policy import AuthPolicy, SystemAuthPolicy, UserAuthPolicy\n",
        "from pkg_resources import get_distribution\n",
        "\n",
        "# Configure resources for automatic authentication\n",
        "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
        "\n",
        "# Add SQL Warehouse and tables for Genie Space\n",
        "# TODO: Update these with your actual warehouse and table names\n",
        "resources.append(DatabricksSQLWarehouse(warehouse_id=\"148ccb90800933a1\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.fact_attrition_snapshots\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.dim_employees\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.fact_compensation\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.fact_performance\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.fact_role_history\"))\n",
        "\n",
        "# Add UC function tools if any\n",
        "for tool in TOOLS:\n",
        "    if isinstance(tool, VectorSearchRetrieverTool):\n",
        "        resources.extend(tool.resources)\n",
        "    elif isinstance(tool, UnityCatalogTool):\n",
        "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
        "\n",
        "# Add Genie Space\n",
        "for agent in EXTERNALLY_SERVED_AGENTS:\n",
        "    if isinstance(agent, Genie):\n",
        "        resources.append(DatabricksGenieSpace(genie_space_id=agent.space_id))\n",
        "    else:\n",
        "        resources.append(DatabricksServingEndpoint(endpoint_name=agent.endpoint_name))\n",
        "\n",
        "# Configure OBO authentication policies\n",
        "# System auth policy: Agent authenticates to these resources automatically\n",
        "systemAuthPolicy = SystemAuthPolicy(resources=resources)\n",
        "\n",
        "# User auth policy: Define API scopes for on-behalf-of user authentication\n",
        "userAuthPolicy = UserAuthPolicy(\n",
        "    api_scopes=[\n",
        "        \"serving.serving-endpoints\",     # For LLM endpoint access\n",
        "        \"sql.warehouses\",                # For Genie SQL warehouse queries (not sql.sql-warehouses)\n",
        "        \"sql.statement-execution\",       # For executing SQL queries on tables\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Log the model with OBO authentication\n",
        "# Note: Don't pass resources separately - they're already in SystemAuthPolicy\n",
        "with mlflow.start_run():\n",
        "    logged_agent_info = mlflow.pyfunc.log_model(\n",
        "        name=\"agent\",\n",
        "        python_model=\"agent.py\",\n",
        "        auth_policy=AuthPolicy(\n",
        "            system_auth_policy=systemAuthPolicy,\n",
        "            user_auth_policy=userAuthPolicy\n",
        "        ),\n",
        "        pip_requirements=[\n",
        "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
        "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
        "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
        "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
        "            f\"langgraph-supervisor=={get_distribution('langgraph-supervisor').version}\",\n",
        "            \"databricks-ai-bridge\",  # Required for OBO authentication\n",
        "        ],\n",
        "    )\n",
        "\n",
        "print(f\"‚úÖ Model logged successfully with OBO authentication!\")\n",
        "print(f\"Run ID: {logged_agent_info.run_id}\")\n",
        "print(f\"Model URI: {logged_agent_info.model_uri}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register to Unity Catalog\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.set_registry_uri(\"databricks-uc\")\n",
        "\n",
        "# TODO: Update these with your catalog, schema, and model name\n",
        "catalog = \"akash_s_demo\"\n",
        "schema = \"talent\"\n",
        "model_name = \"talent_agent_v1\"\n",
        "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
        "\n",
        "# Register the model\n",
        "uc_registered_model_info = mlflow.register_model(\n",
        "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model registered to Unity Catalog!\")\n",
        "print(f\"Model: {UC_MODEL_NAME}\")\n",
        "print(f\"Version: {uc_registered_model_info.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the Agent\n",
        "\n",
        "Deploy the agent to a serving endpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from databricks import agents\n",
        "\n",
        "# Deploy the agent\n",
        "deployment_info = agents.deploy(\n",
        "    UC_MODEL_NAME, \n",
        "    uc_registered_model_info.version,\n",
        "    tags={\"enhanced\": \"with_summary\"},\n",
        "    deploy_feedback_model=False\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ DEPLOYMENT INITIATED\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nYour agent with enhanced summarization is being deployed!\")\n",
        "print(\"\\nüìä What to expect:\")\n",
        "print(\"  ‚Ä¢ Natural language summaries from Llama 3.1\")\n",
        "print(\"  ‚Ä¢ Structured tables from Genie\")\n",
        "print(\"  ‚Ä¢ Both in a single response\")\n",
        "print(\"\\nThis deployment can take up to 15 minutes.\")\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Output\n",
        "\n",
        "### Question: \"Give me attrition rates for each BU\"\n",
        "\n",
        "**What you'll get:**\n",
        "\n",
        "```\n",
        "Sales department has the highest attrition rate at 15.2%, significantly above the 8.1% company average.\n",
        "Engineering maintains the strongest retention at 6.3%, indicating effective retention programs in technical roles.\n",
        "\n",
        "| Department  | Attrition Rate | Employee Count | Avg Tenure |\n",
        "|-------------|----------------|----------------|------------|\n",
        "| Sales       | 15.2%          | 450            | 2.3 years  |\n",
        "| Support     | 12.8%          | 320            | 2.8 years  |\n",
        "| Marketing   | 10.5%          | 180            | 3.2 years  |\n",
        "| Operations  | 9.2%           | 280            | 3.8 years  |\n",
        "| Engineering | 6.3%           | 520            | 4.5 years  |\n",
        "```\n",
        "\n",
        "**In your Dash app, this will display as:**\n",
        "- ‚úÖ **2-line summary** at the top (easy to read)\n",
        "- ‚úÖ **Formatted table** below (with proper styling)\n",
        "- ‚úÖ **Agent badge** showing which agent answered\n",
        "\n",
        "## Key Features\n",
        "\n",
        "‚úÖ **Concise** - Exactly 2 lines of summary, no fluff  \n",
        "‚úÖ **Specific** - Uses actual numbers from the data  \n",
        "‚úÖ **Complete** - Full table preserved for detailed analysis  \n",
        "‚úÖ **Frontend Ready** - Dash app already parses and displays this format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

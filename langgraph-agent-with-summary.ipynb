{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent System with Genie + LLM Summarization\n",
        "\n",
        "This notebook creates a multi-agent system where:\n",
        "1. **Genie Agent** provides structured data (tables, statistics)\n",
        "2. **Supervisor Agent** (Llama 3.1) creates natural language summaries\n",
        "3. **Output includes BOTH** the table and the summary\n",
        "\n",
        "## Prerequisites\n",
        "- Genie Space created and configured\n",
        "- Databricks serving endpoint access\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U -qqq langgraph-supervisor==0.0.30 mlflow[databricks] databricks-langchain databricks-agents uv \n",
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Multi-Agent System with Explicit Graph Structure\n",
        "\n",
        "### Graph Architecture (Nodes and Edges):\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ User Question‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "       ‚îÇ\n",
        "       ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Genie Node  ‚îÇ  ‚Üê Queries data, returns table\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "       ‚îÇ (Forced Edge)\n",
        "       ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇSupervisor Node‚îÇ ‚Üê Creates 2-line summary + preserves table\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "       ‚îÇ\n",
        "       ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ User Response‚îÇ  ‚Üê Returns summary + table\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### Key Features:\n",
        "- ‚úÖ **Explicit routing**: Genie output ALWAYS goes to Supervisor\n",
        "- ‚úÖ **No conditional logic**: Simple linear flow\n",
        "- ‚úÖ **Guaranteed format**: Supervisor always provides 2-line summary + table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile agent.py\n",
        "import json\n",
        "from typing import Generator, Literal\n",
        "from uuid import uuid4\n",
        "\n",
        "import mlflow\n",
        "from databricks_langchain import (\n",
        "    ChatDatabricks,\n",
        "    DatabricksFunctionClient,\n",
        "    UCFunctionToolkit,\n",
        "    set_uc_function_client,\n",
        ")\n",
        "from databricks_langchain.genie import GenieAgent\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain.agents import create_agent\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "from langgraph_supervisor import create_supervisor\n",
        "from mlflow.pyfunc import ResponsesAgent\n",
        "from mlflow.types.responses import (\n",
        "    ResponsesAgentRequest,\n",
        "    ResponsesAgentResponse,\n",
        "    ResponsesAgentStreamEvent,\n",
        "    output_to_responses_items_stream,\n",
        "    to_chat_completions_input,\n",
        ")\n",
        "from pydantic import BaseModel\n",
        "\n",
        "client = DatabricksFunctionClient()\n",
        "set_uc_function_client(client)\n",
        "\n",
        "########################################\n",
        "# Agent Configuration Models\n",
        "########################################\n",
        "\n",
        "GENIE = \"genie\"\n",
        "\n",
        "\n",
        "class ServedSubAgent(BaseModel):\n",
        "    endpoint_name: str\n",
        "    name: str\n",
        "    task: Literal[\"agent/v1/responses\", \"agent/v1/chat\", \"agent/v2/chat\"]\n",
        "    description: str\n",
        "\n",
        "\n",
        "class Genie(BaseModel):\n",
        "    space_id: str\n",
        "    name: str\n",
        "    task: str = GENIE\n",
        "    description: str\n",
        "\n",
        "\n",
        "class InCodeSubAgent(BaseModel):\n",
        "    tools: list[str]\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "\n",
        "TOOLS = []\n",
        "\n",
        "\n",
        "def stringify_content(state):\n",
        "    \"\"\"Convert content to string format for processing\"\"\"\n",
        "    msgs = state[\"messages\"]\n",
        "    if isinstance(msgs[-1].content, list):\n",
        "        msgs[-1].content = json.dumps(msgs[-1].content, indent=4)\n",
        "    return {\"messages\": msgs}\n",
        "\n",
        "\n",
        "########################################\n",
        "# Create Custom LangGraph with Explicit Nodes and Edges\n",
        "########################################\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "\n",
        "\n",
        "# Define the state structure\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, operator.add]\n",
        "\n",
        "\n",
        "def create_langgraph_with_nodes(\n",
        "    llm: Runnable,\n",
        "    externally_served_agents: list[ServedSubAgent] = [],\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a LangGraph with explicit nodes and edges:\n",
        "    - User Question ‚Üí Genie Node (gets data)\n",
        "    - Genie Response ‚Üí Supervisor Node (summarizes)\n",
        "    - Supervisor Response ‚Üí END (returns to user)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create Genie agent\n",
        "    genie_agent = None\n",
        "    for agent in externally_served_agents:\n",
        "        if isinstance(agent, Genie):\n",
        "            genie_agent = GenieAgent(\n",
        "                genie_space_id=agent.space_id,\n",
        "                genie_agent_name=agent.name,\n",
        "                description=agent.description,\n",
        "            )\n",
        "            genie_agent.name = agent.name\n",
        "            break\n",
        "    \n",
        "    if not genie_agent:\n",
        "        raise ValueError(\"Genie agent is required\")\n",
        "    \n",
        "    # Define Genie node function\n",
        "    def genie_node(state: AgentState):\n",
        "        \"\"\"Genie node - queries data and returns structured results\"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        \n",
        "        print(f\"DEBUG Genie - Input messages: {len(messages)}\")\n",
        "        \n",
        "        # Invoke Genie\n",
        "        response = genie_agent.invoke({\"messages\": messages})\n",
        "        \n",
        "        print(f\"DEBUG Genie - Response type: {type(response)}\")\n",
        "        print(f\"DEBUG Genie - Response keys: {response.keys() if isinstance(response, dict) else 'Not a dict'}\")\n",
        "        \n",
        "        # GenieAgent returns the response differently - check for 'output' or last message\n",
        "        genie_output = None\n",
        "        \n",
        "        # Try to get the actual Genie response\n",
        "        if isinstance(response, dict):\n",
        "            # Check if there's an 'output' field (common in agent responses)\n",
        "            if 'output' in response:\n",
        "                genie_output = response['output']\n",
        "                print(f\"DEBUG Genie - Found 'output' field: {str(genie_output)[:200]}\")\n",
        "            # Check if messages were appended\n",
        "            elif 'messages' in response and len(response['messages']) > len(messages):\n",
        "                new_msgs = response['messages'][len(messages):]\n",
        "                genie_output = new_msgs[-1] if new_msgs else None\n",
        "                print(f\"DEBUG Genie - Found new messages: {len(new_msgs)}\")\n",
        "            # Otherwise, get the last message which should have Genie's response\n",
        "            elif 'messages' in response and response['messages']:\n",
        "                last_msg = response['messages'][-1]\n",
        "                genie_output = last_msg\n",
        "                print(f\"DEBUG Genie - Using last message: {type(last_msg)}\")\n",
        "        \n",
        "        # Convert to AIMessage if needed\n",
        "        if genie_output:\n",
        "            if isinstance(genie_output, str):\n",
        "                genie_message = AIMessage(content=genie_output, name=\"genie\")\n",
        "            elif hasattr(genie_output, 'content'):\n",
        "                genie_message = AIMessage(content=genie_output.content, name=\"genie\")\n",
        "            elif isinstance(genie_output, dict) and 'content' in genie_output:\n",
        "                genie_message = AIMessage(content=genie_output['content'], name=\"genie\")\n",
        "            else:\n",
        "                genie_message = AIMessage(content=str(genie_output), name=\"genie\")\n",
        "            \n",
        "            print(f\"DEBUG Genie - Created AIMessage with content length: {len(genie_message.content)}\")\n",
        "            return {\"messages\": [genie_message]}\n",
        "        else:\n",
        "            print(\"ERROR Genie - No output found!\")\n",
        "            return {\"messages\": [AIMessage(content=\"Genie returned no data.\", name=\"genie\")]}\n",
        "    \n",
        "    # Define Supervisor node function\n",
        "    def supervisor_node(state: AgentState):\n",
        "        \"\"\"Supervisor node - summarizes Genie's data into 2-line summary + table\"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        \n",
        "        # Get ALL messages - find the one from Genie (should be AI message after user message)\n",
        "        genie_response = \"\"\n",
        "        \n",
        "        # Look for the last AI message (from Genie)\n",
        "        for msg in reversed(messages):\n",
        "            if hasattr(msg, 'content') and msg.content:\n",
        "                # Check if it's an AI message and has actual content\n",
        "                if isinstance(msg, AIMessage) or (hasattr(msg, 'type') and msg.type == 'ai'):\n",
        "                    content = str(msg.content)\n",
        "                    # Skip if it's too short or empty\n",
        "                    if content and len(content.strip()) > 10:\n",
        "                        genie_response = content\n",
        "                        break\n",
        "        \n",
        "        # Debug: Print what we got from Genie\n",
        "        print(f\"DEBUG - Messages count: {len(messages)}\")\n",
        "        print(f\"DEBUG - Genie response length: {len(genie_response) if genie_response else 0}\")\n",
        "        if genie_response:\n",
        "            print(f\"DEBUG - Genie response preview: {genie_response[:200]}...\")\n",
        "        \n",
        "        if not genie_response or len(genie_response.strip()) < 10:\n",
        "            # If still no response, get the full state for debugging\n",
        "            error_msg = f\"No data received from Genie. Messages in state: {len(messages)}\"\n",
        "            print(f\"ERROR: {error_msg}\")\n",
        "            for i, msg in enumerate(messages):\n",
        "                print(f\"  Message {i}: type={type(msg).__name__}, has_content={hasattr(msg, 'content')}\")\n",
        "                if hasattr(msg, 'content'):\n",
        "                    content_preview = str(msg.content)[:100]\n",
        "                    print(f\"    Content preview: {content_preview}\")\n",
        "            return {\"messages\": [AIMessage(content=error_msg)]}\n",
        "        \n",
        "        # Create supervisor prompt - be VERY explicit\n",
        "        system_prompt = \"\"\"You are a data analyst. Your job is to write a 2-line summary and include the original table.\n",
        "\n",
        "OUTPUT FORMAT (copy exactly):\n",
        "[Line 1: Key finding with number]\n",
        "[Line 2: Second insight]\n",
        "\n",
        "[PASTE THE ORIGINAL TABLE HERE]\n",
        "\n",
        "EXAMPLE:\n",
        "Sales has highest attrition at 15.2%, above the 8.1% average.\n",
        "Engineering shows best retention at 6.3% with effective programs.\n",
        "\n",
        "| Department | Rate  | Count |\n",
        "|------------|-------|-------|\n",
        "| Sales      | 15.2% | 450   |\n",
        "| Engineering| 6.3%  | 520   |\n",
        "\n",
        "RULES:\n",
        "- Write EXACTLY 2 short lines analyzing the data\n",
        "- Add blank line\n",
        "- Copy the COMPLETE original table unchanged\n",
        "- That's it - nothing else\"\"\"\n",
        "        \n",
        "        # Create messages for LLM with explicit instruction\n",
        "        user_prompt = f\"\"\"Here is the data with a table:\n",
        "\n",
        "{genie_response}\n",
        "\n",
        "Instructions:\n",
        "1. Write 2 lines summarizing the key findings\n",
        "2. Include the complete table from above\n",
        "\n",
        "Your response:\"\"\"\n",
        "        \n",
        "        supervisor_messages = [\n",
        "            SystemMessage(content=system_prompt),\n",
        "            HumanMessage(content=user_prompt)\n",
        "        ]\n",
        "        \n",
        "        # Get summary from LLM\n",
        "        summary_response = llm.invoke(supervisor_messages)\n",
        "        \n",
        "        # Combine summary with original table to ensure table is preserved\n",
        "        final_response = summary_response.content\n",
        "        \n",
        "        # If the table isn't in the response, append it\n",
        "        if '|' not in final_response and '|' in genie_response:\n",
        "            print(\"DEBUG - Table not in LLM response, appending original table\")\n",
        "            final_response = f\"{final_response}\\n\\n{genie_response}\"\n",
        "        \n",
        "        print(f\"DEBUG Supervisor - Final response length: {len(final_response)}\")\n",
        "        print(f\"DEBUG Supervisor - Response preview: {final_response[:300]}\")\n",
        "        \n",
        "        # Return as a single message with the summary content\n",
        "        return {\"messages\": [AIMessage(content=final_response, name=\"supervisor\")]}\n",
        "    \n",
        "    # Build the graph with explicit nodes and edges\n",
        "    workflow = StateGraph(AgentState)\n",
        "    \n",
        "    # Add nodes\n",
        "    workflow.add_node(\"genie\", genie_node)\n",
        "    workflow.add_node(\"supervisor\", supervisor_node)\n",
        "    \n",
        "    # Define edges: User ‚Üí Genie ‚Üí Supervisor ‚Üí END\n",
        "    workflow.set_entry_point(\"genie\")\n",
        "    workflow.add_edge(\"genie\", \"supervisor\")  # Genie ALWAYS goes to Supervisor\n",
        "    workflow.add_edge(\"supervisor\", END)       # Supervisor ALWAYS returns to user\n",
        "    \n",
        "    return workflow.compile()\n",
        "\n",
        "\n",
        "##########################################\n",
        "# Wrap LangGraph Supervisor as a ResponsesAgent\n",
        "##########################################\n",
        "\n",
        "\n",
        "class LangGraphResponsesAgent(ResponsesAgent):\n",
        "    def __init__(self, agent: CompiledStateGraph):\n",
        "        self.agent = agent\n",
        "\n",
        "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
        "        outputs = [\n",
        "            event.item\n",
        "            for event in self.predict_stream(request)\n",
        "            if event.type == \"response.output_item.done\"\n",
        "        ]\n",
        "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
        "\n",
        "    def predict_stream(\n",
        "        self,\n",
        "        request: ResponsesAgentRequest,\n",
        "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
        "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
        "        seen_ids = set()\n",
        "        \n",
        "        # Track input message IDs to skip them\n",
        "        input_msg_ids = set()\n",
        "\n",
        "        for _, events in self.agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\"]):\n",
        "            node_name = tuple(events.keys())[0] if events else \"unknown\"\n",
        "            \n",
        "            # Get messages from this node\n",
        "            new_msgs = []\n",
        "            for v in events.values():\n",
        "                for msg in v.get(\"messages\", []):\n",
        "                    if msg.id not in seen_ids and msg.id not in input_msg_ids:\n",
        "                        new_msgs.append(msg)\n",
        "                        seen_ids.add(msg.id)\n",
        "            \n",
        "            # Emit node name tag\n",
        "            if new_msgs:\n",
        "                yield ResponsesAgentStreamEvent(\n",
        "                    type=\"response.output_item.done\",\n",
        "                    item=self.create_text_output_item(\n",
        "                        text=f\"<name>{node_name}</name>\", id=str(uuid4())\n",
        "                    ),\n",
        "                )\n",
        "                \n",
        "                # Emit the actual messages\n",
        "                yield from output_to_responses_items_stream(new_msgs)\n",
        "\n",
        "\n",
        "#######################################################\n",
        "# Configure Foundation Model and Sub-Agents\n",
        "#######################################################\n",
        "\n",
        "# Foundation model for supervisor (will generate summaries)\n",
        "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
        "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
        "\n",
        "# Configure your Genie Space\n",
        "EXTERNALLY_SERVED_AGENTS = [\n",
        "    Genie(\n",
        "        space_id=\"01f0c9f705201d14b364f5daf28bb639\",  # TODO: Update with your Genie Space ID\n",
        "        name=\"talent_genie\",\n",
        "        description=\"Analyzes talent stability, mobility patterns, attrition risk, and workforce trends. Provides structured data including statistics, tables, and detailed breakdowns by department, role, tenure, and other dimensions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Optional: Add UC function-calling agents\n",
        "IN_CODE_AGENTS = []\n",
        "\n",
        "#################################################\n",
        "# Create Graph with Explicit Nodes and Edges\n",
        "#################################################\n",
        "\n",
        "# Create the graph: User ‚Üí Genie ‚Üí Supervisor ‚Üí END\n",
        "supervisor = create_langgraph_with_nodes(llm, EXTERNALLY_SERVED_AGENTS)\n",
        "\n",
        "print(\"‚úì Graph created with explicit flow:\")\n",
        "print(\"  User Question ‚Üí Genie Node ‚Üí Supervisor Node ‚Üí User Response\")\n",
        "\n",
        "mlflow.langchain.autolog()\n",
        "AGENT = LangGraphResponsesAgent(supervisor)\n",
        "mlflow.models.set_model(AGENT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Graph Structure\n",
        "\n",
        "Display the node and edge structure of the LangGraph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agent import supervisor\n",
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    # Try to generate graph visualization\n",
        "    graph_image = supervisor.get_graph().draw_mermaid_png()\n",
        "    display(Image(graph_image))\n",
        "    print(\"‚úì Graph visualization displayed above\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not generate graph image: {e}\")\n",
        "    print(\"\\nGraph Structure (text):\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"START\")\n",
        "    print(\"  ‚Üì\")\n",
        "    print(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
        "    print(\"‚îÇ genie      ‚îÇ  ‚Üê Queries Genie Space for data\")\n",
        "    print(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
        "    print(\"      ‚Üì (forced edge)\")\n",
        "    print(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
        "    print(\"‚îÇ supervisor ‚îÇ  ‚Üê Creates 2-line summary + table\")\n",
        "    print(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
        "    print(\"      ‚Üì\")\n",
        "    print(\"    END\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nFlow:\")\n",
        "    print(\"1. User question ‚Üí genie node\")\n",
        "    print(\"2. Genie returns table ‚Üí supervisor node\")\n",
        "    print(\"3. Supervisor adds 2-line summary ‚Üí END\")\n",
        "    print(\"4. Response: Summary + Table ‚Üí User\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Agent\n",
        "\n",
        "Test the agent locally before deploying. You should see:\n",
        "1. **Summary** from the supervisor (natural language insights)\n",
        "2. **Table** from Genie (structured data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agent import AGENT\n",
        "\n",
        "# Test with a question that will require Genie to query data\n",
        "input_example = {\n",
        "    \"input\": [\n",
        "        {\"role\": \"user\", \"content\": \"Which department has the highest attrition rate?\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Get the response\n",
        "response = AGENT.predict(input_example)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test streaming to see the flow\n",
        "print(\"=\" * 80)\n",
        "print(\"STREAMING OUTPUT (shows agent handoffs and responses)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for event in AGENT.predict_stream(input_example):\n",
        "    output = event.model_dump(exclude_none=True)\n",
        "    \n",
        "    # Extract and display content\n",
        "    if 'item' in output and 'content' in output['item']:\n",
        "        for content_item in output['item']['content']:\n",
        "            if 'text' in content_item:\n",
        "                text = content_item['text']\n",
        "                \n",
        "                # Highlight agent names\n",
        "                if text.startswith('<name>'):\n",
        "                    print(f\"\\n{'='*60}\")\n",
        "                    print(f\"‚ûú Agent: {text}\")\n",
        "                    print(f\"{'='*60}\\n\")\n",
        "                else:\n",
        "                    print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log the Agent to MLflow\n",
        "\n",
        "Log the agent with automatic authentication for Databricks resources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from agent import EXTERNALLY_SERVED_AGENTS, LLM_ENDPOINT_NAME, TOOLS, Genie\n",
        "from databricks_langchain import UnityCatalogTool, VectorSearchRetrieverTool\n",
        "from mlflow.models.resources import (\n",
        "    DatabricksFunction,\n",
        "    DatabricksGenieSpace,\n",
        "    DatabricksServingEndpoint,\n",
        "    DatabricksSQLWarehouse,\n",
        "    DatabricksTable\n",
        ")\n",
        "from pkg_resources import get_distribution\n",
        "\n",
        "# Configure resources for automatic authentication\n",
        "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
        "\n",
        "# Add SQL Warehouse and tables for Genie Space\n",
        "# TODO: Update these with your actual warehouse and table names\n",
        "resources.append(DatabricksSQLWarehouse(warehouse_id=\"148ccb90800933a1\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.fact_attrition_snapshots\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.dim_employees\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.fact_compensation\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.fact_performance\"))\n",
        "resources.append(DatabricksTable(table_name=\"akash_s_demo.talent.fact_role_history\"))\n",
        "\n",
        "# Add UC function tools if any\n",
        "for tool in TOOLS:\n",
        "    if isinstance(tool, VectorSearchRetrieverTool):\n",
        "        resources.extend(tool.resources)\n",
        "    elif isinstance(tool, UnityCatalogTool):\n",
        "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
        "\n",
        "# Add Genie Space\n",
        "for agent in EXTERNALLY_SERVED_AGENTS:\n",
        "    if isinstance(agent, Genie):\n",
        "        resources.append(DatabricksGenieSpace(genie_space_id=agent.space_id))\n",
        "    else:\n",
        "        resources.append(DatabricksServingEndpoint(endpoint_name=agent.endpoint_name))\n",
        "\n",
        "# Log the model\n",
        "with mlflow.start_run():\n",
        "    logged_agent_info = mlflow.pyfunc.log_model(\n",
        "        name=\"agent\",\n",
        "        python_model=\"agent.py\",\n",
        "        resources=resources,\n",
        "        pip_requirements=[\n",
        "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
        "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
        "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
        "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
        "            f\"langgraph-supervisor=={get_distribution('langgraph-supervisor').version}\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "print(f\"‚úÖ Model logged successfully!\")\n",
        "print(f\"Run ID: {logged_agent_info.run_id}\")\n",
        "print(f\"Model URI: {logged_agent_info.model_uri}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register to Unity Catalog\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.set_registry_uri(\"databricks-uc\")\n",
        "\n",
        "# TODO: Update these with your catalog, schema, and model name\n",
        "catalog = \"akash_s_demo\"\n",
        "schema = \"talent\"\n",
        "model_name = \"mobility_attrition_with_summary\"\n",
        "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
        "\n",
        "# Register the model\n",
        "uc_registered_model_info = mlflow.register_model(\n",
        "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model registered to Unity Catalog!\")\n",
        "print(f\"Model: {UC_MODEL_NAME}\")\n",
        "print(f\"Version: {uc_registered_model_info.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the Agent\n",
        "\n",
        "Deploy the agent to a serving endpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from databricks import agents\n",
        "\n",
        "# Deploy the agent\n",
        "deployment_info = agents.deploy(\n",
        "    UC_MODEL_NAME, \n",
        "    uc_registered_model_info.version,\n",
        "    tags={\"enhanced\": \"with_summary\"},\n",
        "    deploy_feedback_model=False\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ DEPLOYMENT INITIATED\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nYour agent with enhanced summarization is being deployed!\")\n",
        "print(\"\\nüìä What to expect:\")\n",
        "print(\"  ‚Ä¢ Natural language summaries from Llama 3.1\")\n",
        "print(\"  ‚Ä¢ Structured tables from Genie\")\n",
        "print(\"  ‚Ä¢ Both in a single response\")\n",
        "print(\"\\nThis deployment can take up to 15 minutes.\")\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Output\n",
        "\n",
        "### Question: \"Give me attrition rates for each BU\"\n",
        "\n",
        "**What you'll get:**\n",
        "\n",
        "```\n",
        "Sales department has the highest attrition rate at 15.2%, significantly above the 8.1% company average.\n",
        "Engineering maintains the strongest retention at 6.3%, indicating effective retention programs in technical roles.\n",
        "\n",
        "| Department  | Attrition Rate | Employee Count | Avg Tenure |\n",
        "|-------------|----------------|----------------|------------|\n",
        "| Sales       | 15.2%          | 450            | 2.3 years  |\n",
        "| Support     | 12.8%          | 320            | 2.8 years  |\n",
        "| Marketing   | 10.5%          | 180            | 3.2 years  |\n",
        "| Operations  | 9.2%           | 280            | 3.8 years  |\n",
        "| Engineering | 6.3%           | 520            | 4.5 years  |\n",
        "```\n",
        "\n",
        "**In your Dash app, this will display as:**\n",
        "- ‚úÖ **2-line summary** at the top (easy to read)\n",
        "- ‚úÖ **Formatted table** below (with proper styling)\n",
        "- ‚úÖ **Agent badge** showing which agent answered\n",
        "\n",
        "## Key Features\n",
        "\n",
        "‚úÖ **Concise** - Exactly 2 lines of summary, no fluff  \n",
        "‚úÖ **Specific** - Uses actual numbers from the data  \n",
        "‚úÖ **Complete** - Full table preserved for detailed analysis  \n",
        "‚úÖ **Frontend Ready** - Dash app already parses and displays this format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
